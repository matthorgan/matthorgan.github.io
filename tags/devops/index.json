[
    {
        "ref": "https://matthorgan.xyz/blog/powershell-type-value-gotchas/",
        "title": "Handling null values in PowerShell and the sneaky gotcha that might catch you out",
        "section": "blog",
        "tags": ["powershell","automation","ci/cd","devops"],
        "date" : "2023.08.16",
        "body": "In this blog post, I\u0026rsquo;ll be covering a fun little null value related \u0026lsquo;gotcha\u0026rsquo; that has caught me out over the years, and the explanation for why it happens. Let\u0026rsquo;s jump right in with how we handle null values and the issues I\u0026rsquo;ve come across in the past.\nHandling Null Values In PowerShell, there are several ways to check for a null value in a variable. One of the standard ways looks like this:\n$MyVariable = $null if ($null -eq $MyVariable) { \u0026#34;The variable is null\u0026#34; } The variable is null Another option is to use the static .NET String class and call the IsNullOrEmpty method like this:\n$MyVariable = $null if ([String]::IsNullOrEmpty($MyVariable)) { \u0026#34;The variable is null\u0026#34; } The variable is null Or, if you want to handle whitespace too, you could use the IsNullOrWhiteSpace method:\n# Whitespace variable that we want to check for $MyVariable = \u0026#39; \u0026#39; if ([String]::IsNullOrWhiteSpace($MyVariable)) { \u0026#34;The variable is null or is whitespace\u0026#34; } The variable is null or is whitespace The Null String \u0026lsquo;Gotcha\u0026rsquo; A lot of the time, I found myself opting for the IsNullOrWhiteSpace option because I\u0026rsquo;d previously been caught out by $null -eq $MyVariable not giving me the result I was expecting. I never fully understood why until recently when I was caught out once again.\nA significant portion of my PowerShell scripts and functions are used in CI/CD pipelines, where environment variables inherited from the CI/CD tool, such as a hostname or username, can be present. Instead of explicitly passing every variable into my scripts/functions, I use a pattern like this:\nfunction New-ExampleFunction { [CmdletBinding()] param ( [Parameter(Mandatory = $false)] [ValidateNotNullOrEmpty()] [String]$MyVariable = $env:VAR_FROM_CI_ENVIRONMENT ) Write-Host $MyVariable } As seen in the above function, the environment variable VAR_FROM_CI_ENVIRONMENT is available within the pipeline of our CI/CD tool. Therefore, we don\u0026rsquo;t need to provide a value for $MyVariable explicitly in this case. This approach offers the flexibility of passing a value to the parameter if desired, while defaulting to the known environment variable otherwise.\nThe issue however, is that outside the scope of our CI/CD environment, what happens if somebody doesn\u0026rsquo;t pass in a value for $MyVariable and they don\u0026rsquo;t have the $env:VAR_FROM_CI_ENVIRONMENT environment variable configured? We need to add an additional bit of validation in our function to ensure that $MyVariable contains a value if one isn\u0026rsquo;t passed in (The [ValidateNotNullOrEmpty()] is only validating values being passed in to $MyVariable, and not something we set as a default value).\nAdding some validation to the above function to check for null can create a nasty little gotcha and this is the main topic for this blog post. I added a traditional $null -eq $MyVariable check to ensure if $MyVariable was null and expected that it would throw a message and stop the function (Assuming nothing had been passed in OR the $env:VAR_FROM_CI_ENVIRONMENT environment variable hadn\u0026rsquo;t been set):\n# This does not produce the expected outcome function New-ExampleFunction { [CmdletBinding()] param ( [Parameter(Mandatory = $false)] [ValidateNotNullOrEmpty()] [String]$MyVariable = $env:VAR_FROM_CI_ENVIRONMENT ) if ($null -eq $MyVariable) { throw \u0026#39;The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please pass in a value or check the env var $env:VAR_FROM_CI_ENVIRONMENT is set\u0026#39; } Write-Host $MyVariable } When you run the above function without passing in a value for $MyVariable, and ensuring the VAR_FROM_CI_ENVIRONMENT env var is not set, nothing happens and we don\u0026rsquo;t see the error message we\u0026rsquo;re expecting.\nWell that\u0026rsquo;s not what I was expecting?! Checking that the env var is definitely not set by running $null -eq $env:VAR_FROM_CI_ENVIRONMENT comes back as True so it definitely IS null. Why isn\u0026rsquo;t PowerShell recognising it as null?\nIf we change how we\u0026rsquo;re checking for null to use the static string class method, look what happens:\n# This DOES produce the expected outcome function New-ExampleFunction { [CmdletBinding()] param ( [Parameter(Mandatory = $false)] [ValidateNotNullOrEmpty()] [String]$MyVariable = $env:VAR_FROM_CI_ENVIRONMENT ) if ([String]::IsNullOrEmpty($MyVariable)) { throw \u0026#39;The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please pass in a value or check the env var $env:VAR_FROM_CI_ENVIRONMENT is set\u0026#39; } Write-Host $MyVariable } Exception: Line | 14 | throw \u0026#39;The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please … | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ | The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please pass in a value or check the env var $env:VAR_FROM_CI_ENVIRONMENT is set This is now working as expected. But why? In the past I used to just change the $null -eq $MyVariable syntax to [String]::IsNullOrWhiteSpace(), wrongly assuming that the variable must have been whitespace but as you can see from the above working code, we\u0026rsquo;re using [String]::IsNullOrEmpty() so that theory has been proved to be incorrect.\nLook what happens when we remove the [String] casting from the function and we go back to using our $null -eq $MyVariable syntax:\nfunction New-ExampleFunction { [CmdletBinding()] param ( [Parameter(Mandatory = $false)] [ValidateNotNullOrEmpty()] # Note the [String] has now been removed $MyVariable = $env:VAR_FROM_CI_ENVIRONMENT ) if ($null -eq $MyVariable) { throw \u0026#39;The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please pass in a value or check the env var $env:VAR_FROM_CI_ENVIRONMENT is set\u0026#39; } Write-Host $MyVariable } We now see the error message that we were hoping to see originally (and that we saw with the [String]::IsNullOrEmpty() method):\nException: Line | 14 | throw \u0026#39;The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please … | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ | The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please pass in a value or check the env var $env:VAR_FROM_CI_ENVIRONMENT is set Conclusion Alrighty then. So it seems that casting the parameter MyVariable to a string is causing our original null comparison to fail. The reason for this is that when we add the [String] to our parameter $MyVariable and we don\u0026rsquo;t pass in a value, it takes the $env:VAR_FROM_CI_ENVIRONMENT and converts it into an empty string. When you strongly type a value type in PowerShell, it will convert $null into whatever the default value is for the type:\n[int]$number = $null $number 0 [bool]$boolean = $null $boolean False [string]$string = $null $string -eq \u0026#39;\u0026#39; True This explains the initial failure of our $null -eq $MyVariable example. PowerShell is converting the empty $MyVariable into the default value for a string and an empty string is not truly null. Using the string class method [String]::IsNullOrEmpty() ensures proper handling of an empty string.\nWhen $MyVariable wasn\u0026rsquo;t strongly typed to any specific type, the $null -eq $MyVariable comparison worked because, at that point, PowerShell didn\u0026rsquo;t know the type and therefore didn\u0026rsquo;t convert it to a default value of any specific type\nAnd there we have it. I can finally sleep a little easier now that I have a clear picture of the potential type-related issues when evaluating $null and why they occur.\n"
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/powershell-handling-native-applications/",
        "title": "Handling Native Applications in PowerShell 7.3+ with $PSNativeCommandErrorActionPreference",
        "section": "blog",
        "tags": ["powershell","automation","devops"],
        "date" : "2023.02.28",
        "body": "I\u0026rsquo;ve previously written a blog post about how to handle Azure CLI errors in PowerShell. The general pattern involves redirecting any error streams to the correct place, and checking for a known exit code. There\u0026rsquo;s an exciting new feature in PowerShell from version 7.3 onwards that makes capturing errors for native applications like the Azure CLI much simpler. Enter: $PSNativeCommandErrorActionPreference\nPSNativeCommandErrorActionPreference is a preference variable that when set to true, allows for native commands to be handled in a more \u0026lsquo;PowerShell\u0026rsquo; way.\nTo enable it, you just need to set $PSNativeCommandErrorActionPreference = $true somewhere in your script, and then you can use a conventional try/catch for your error handling. Check out this before/after example:\nBefore Assign variable to output Redirect the error stream and assign to variable $err to ensure any error(s) get captured If the $LASTEXITCODE isn\u0026rsquo;t a success, throw the contents of the $err variable $err = $($podList = kubectl get pods) 2\u0026gt;\u0026amp;1 if ($LASTEXITCODE -ne 0) { throw $err } After Set the $PSNativeCommandUseErrorActionPreference to true Set $ErrorActionPreference to Stop Handle the error in a traditional try/catch and it\u0026rsquo;ll throw if there\u0026rsquo;s a non-zero exit code (Note you may still want to redirect the error stream depending on the command) $PSNativeCommandUseErrorActionPreference = $true $ErrorActionPreference = \u0026#39;Stop\u0026#39; try { $podList = kubectl get pods } catch { throw } "
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/powershell-unexpected-character-error/",
        "title": "PowerShell Unexpected Character Error",
        "section": "blog",
        "tags": ["powershell","automation","azure","ci/cd","devops"],
        "date" : "2023.01.05",
        "body": "Over the years, I\u0026rsquo;ve had a few different Unexpected character encountered errors in PowerShell and for the longest time, they were always a real pain to troubleshoot and looked like a bug. Once you know the issue though, it\u0026rsquo;s a fairly simple one to solve. The latest one I\u0026rsquo;ve come across is running the following command from our CI/CD Linux build container to a Windows 2012 machine hosted in Azure:\nInvoke-AzVMRunCommand -ResourceGroupName $VMResourceGroupName -Name $VMName -CommandId 'RunPowerShellScript' -ScriptPath \u0026quot;tests/gather-vm-info.ps1\u0026quot;\nThe above command is using the Azure VM run command to run the contents of tests/gather-vm-info.ps1 on the remote Windows 2012 VM. It should be noted, I was running the exact same command on Windows 2016, and Windows 2019 VMs without any issues. The error I was seeing was the following:\nJsonReaderException: Unexpected character encountered while parsing value: W. Path \u0026#39;\u0026#39;, line 0, position 0. ArgumentException: Conversion from JSON failed with error: Unexpected character encountered while parsing value: W. Path \u0026#39;\u0026#39;, line 0, position 0. at \u0026lt;ScriptBlock\u0026gt;, /builds/golden-images/tests/gather-vm-info.ps1:49 Initially looking at this error, it felt \u0026lsquo;buggy\u0026rsquo; considering there was no \u0026lsquo;W\u0026rsquo; in my output when I ran the tests/gather-vm-info.ps1 code individually on the VM itself. I assumed it must be an issue with the Invoke-AzVMRunCommand but actually, when you look closer at the output, it\u0026rsquo;s easy to see what the issue is.\nValue[0] : Code : ComponentStatus/StdOut/succeeded Level : Info DisplayStatus : Provisioning succeeded Message : WARNING: Ping to \u0026lt;redacted URL\u0026gt; failed -- Status: TimedOut { \u0026#34;Output1\u0026#34;: true, \u0026#34;Output2\u0026#34;: true, \u0026#34;Output3\u0026#34;: true, \u0026#34;Output4\u0026#34;: true, \u0026#34;Output5\u0026#34;: false, \u0026#34;Output6\u0026#34;: true, \u0026#34;Output7\u0026#34;: true, \u0026#34;Output8\u0026#34;: false, \u0026#34;Output9\u0026#34;: false } As you can see, on investigating the Value.Message property of the object returned from Invoke-AzVMRunCommand, we\u0026rsquo;ve got a warning message at the very start of the output before our custom JSON. Suddenly, the W character in the initial error makes sense. In our scenario, Value.Message returns all output including any warning messages and therefore whilst our JSON is being returned correctly, we need to ensure that any warning messages are properly handled or suppressed to ensure that our output only returns JSON ready to be converted.\nSo, the next time you see a weird unexpected parsing error, have a look at what the starting character is. If it\u0026rsquo;s a W, there\u0026rsquo;s a good chance you could be trying to parse something that has captured one of the PowerShell output streams along with your expected output.\n"
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/azcli-error-handling-in-powershell/",
        "title": "Azure CLI Error Handling in PowerShell",
        "section": "blog",
        "tags": ["powershell","automation","azcli","ci/cd","devops"],
        "date" : "2021.09.16",
        "body": "The Azure CLI (azcli) is an extremely useful tool in interacting with the Azure platform. I\u0026rsquo;ve been favouring it over the Azure PowerShell modules recently due to many of the commands being idempotent (E.g. not having to check if something already exists before trying to create can be a nice time saver). One of the downsides to using the azcli in PowerShell scripts however, is that you can\u0026rsquo;t handle errors like you would with a typical PowerShell cmdlet.\nBefore we get into the code, it\u0026rsquo;s worth saying that to filter data with the Azure CLI, you\u0026rsquo;ve got two options. Option one is to use the JMESPath query parameter e.g. in the following example az vm show --resource-group QueryDemo --name TestVM --query \u0026quot;osProfile.linuxConfiguration.ssh.publicKeys\u0026quot;. The default output from the Azure CLI is JSON and so this query is targeting a nested property publicKeys which in JSON would look something like \u0026quot;osProfile:\u0026quot; { \u0026quot;linuxConfiguration\u0026quot;: {\u0026quot;ssh\u0026quot;: { \u0026quot;publicKeys\u0026quot;: [{ \u0026quot;someData\u0026quot;: \u0026quot;someDataHere\u0026quot; }]} } }. The option I prefer however, is to use PowerShell\u0026rsquo;s ConvertFrom-Json cmdlet which brings us into our warm and cosy PowerShell world. So the same filter would be: (az vm show --resource-group QueryDemo --name TestVM | ConvertFrom-Json).osProfile.linuxConfiguration.ssh.publicKeys. I\u0026rsquo;ll be adding the ConvertFrom-Json cmdlet to the examples as this is more representitive of how I\u0026rsquo;d actually use the azcli.\nConsider the below examples with an App Registration that doesn\u0026rsquo;t exist:\ntry { $appReg = Get-AzureADApplication -ObjectId \u0026#39;NotARealObjectId\u0026#39; -ErrorAction \u0026#39;Stop\u0026#39; } catch { Write-Error \u0026#34;Uh oh, we\u0026#39;ve got an error here...\u0026#34; throw } try { $appReg = az ad app show --id \u0026#39;NotARealObjectId\u0026#39; | ConvertFrom-Json } catch { Write-Error \u0026#34;Uh oh, we\u0026#39;ve got an error here...\u0026#34; throw } In the first example, we get a lovely handled error and a custom message but in the azcli example, nothing gets thrown because the azcli is an external tool and PowerShell doesn\u0026rsquo;t natively know what to do with it.\nWe can improve things by making use of the $LASTEXITCODE which will give you the exit code of the last ran command.\n$appReg = az ad sp show --id \u0026#39;NotARealObjectId\u0026#39; | ConvertFrom-Json if ($LASTEXITCODE -ne 0) { Write-Error \u0026#34;Uh oh, we\u0026#39;ve got an error here...\u0026#34; -ErrorAction \u0026#39;Stop\u0026#39; } The above example will display our custom error message and halt the script if we get an exit code that isn\u0026rsquo;t 0. You\u0026rsquo;ll also find that the azcli spits out its own error but at the moment this isn\u0026rsquo;t information that we can capture within the constraints of our own error handling - it\u0026rsquo;ll just show the error as soon as the command isn\u0026rsquo;t successful.\nSo how do we capture the azcli error within our own error handling? We can utilise a bit of stream redirection and a subexpression to achieve a better result:\n$errOutput = $($appReg = \u0026amp; {az ad sp show --id \u0026#39;NotARealObjectId\u0026#39; | ConvertFrom-Json}) 2\u0026gt;\u0026amp;1 if ($errOutput) { Write-Error \u0026#34;Uh oh, we\u0026#39;ve got an error here...\u0026#34; -ErrorAction \u0026#39;Continue\u0026#39; throw $errOutput } In this final example, we\u0026rsquo;re executing the azcli command using the ampersand operator and capturing the variable output in the $appReg variable like before. However, this time we\u0026rsquo;re using a subexpression and redirecting the error stream of that subexpression to the success stream and capturing it in a variable called $errOutput. Now that we\u0026rsquo;ve got the error in a variable, we can display it and handle it however we like.\nOne thing to note is make sure you\u0026rsquo;re aware of what output the azcli command you\u0026rsquo;re using gives you. Some azcli commands send useful non-error commands to the error stream and therefore redirecting it like we\u0026rsquo;ve done above will trigger an error. If you wanted to, you could technically use a hybrid of the above two methods to ensure this doesn\u0026rsquo;t trip you up:\n$errOutput = $($appReg = \u0026amp; {az ad sp show --id \u0026#39;NotARealObjectId\u0026#39; | ConvertFrom-Json}) 2\u0026gt;\u0026amp;1 if ($LASTEXITCODE -ne 0) { Write-Error \u0026#34;Uh oh, we\u0026#39;ve got an error here...\u0026#34; -ErrorAction \u0026#39;Continue\u0026#39; throw $errOutput } "
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/chart-testing-gitlabci-error/",
        "title": "Error using chart-testing in GitLab Pipeline",
        "section": "blog",
        "tags": ["gitlab","automation","helm","chart-testing","ci/cd","devops"],
        "date" : "2021.01.12",
        "body": "We\u0026rsquo;ve recently been moving over from Jenkins to GitLab and as part of this, I was creating a validation pipeline for our Helm charts using chart-testing. When I tried to use the tool for some basic linting using ct lint, I kept getting the error:\nError: Error linting charts: Error identifying charts to process: Error running process: exit status 128.\nHowever, when I ran ct lint --all, everything seemed to work OK and the charts were analysed as expected. Looking further into the documentation for chart-testing, if you run --all, it\u0026rsquo;ll ignore any git functionality and just analyse the charts. Without that parameter, it\u0026rsquo;ll compare with what is already in source control and only analyse the charts that have differences.\nBy default, Gitlab CI does a shallow clone which means there is no git history for the tool to look at. Unfortunately, the exit status 128 that you get back from the tool didn\u0026rsquo;t help pinpoint it but once you disable the shallow clone within GitLab CI, chart-testing will now have the git information it needs to only analyse the charts that have changed.\nFYI, to disable shallow clone in GitLab CI, add an environment variable called GIT_DEPTH and set it to 0:\nvariables: GIT_DEPTH: 0 "
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/appveyor-to-github-actions/",
        "title": "Moving from AppVeyor to GitHub Actions",
        "section": "blog",
        "tags": ["appveyor","github","actions","devops","ci/cd"],
        "date" : "2020.11.26",
        "body": "It\u0026rsquo;s been over three and a half years since I first created my blog with Hugo and AppVeyor. Since then, I\u0026rsquo;ve been fortunate (or unfortunate depending on the tool!) to use lots of other CI/CD tools such as GitLab CI, Jenkins, Bamboo, Azure DevOps. GitHub Actions is one tool that I hadn\u0026rsquo;t used yet and it\u0026rsquo;s been on my radar for a while so why not take the opportunity to update the blog and the CI pipeline?\nOne of the main wins for me with GitHub Actions is the integration with GitHub. Having your code and your CI pipeline in one familiar GUI makes for a really nice experience. There\u0026rsquo;s also a great VSCode plugin I found called cschleiden.vscode-github-actions which provides a really nice view of all your pipeline builds (or workflows as they\u0026rsquo;re called in GitHub) like this:\nAnother great benefit of GitHub Actions is the marketplace that contains thousands of actions that can help automate your workflow.\nThe first thing I was going to do before I got started with GitHub Actions was to move my old inline code from the AppVeyor build file and into its own script. Over the years, I\u0026rsquo;ve realised that pipelines can get wildly out of hand if you don\u0026rsquo;t abstract as much code away from the pipeline syntax as possible. This makes your pipelines way easier to read and understand and allows for them to be easily portable into other CI tools if required.\nAfter some initial testing with the Windows 2019 runner, I decided to swap over to Linux as the builds went from around 30s in AppVeyor to a whole 3m30 in GitHub Actions.\nBefore I started to refactor my code into a little bash script, I thought I\u0026rsquo;d have a quick look at what community Actions were available to potentially simplify my workflow and lo and behold, it couldn\u0026rsquo;t have been simpler.\nThere\u0026rsquo;s a Hugo Action available from here which installs Hugo at the version you specify. On top of that, there\u0026rsquo;s a GitHub Pages Action which allows you to deploy your static content to your gh-pages branch or whichever branch your GitHub Pages is setup to point to.\nThis therefore meant that I didn\u0026rsquo;t actually need to use any scripts or any custom code at all and my workflow YAML file is wonderfully simple. My code is almost exactly the same as one of the examples in the Hugo Actions repo - everything just worked - what a joy it is to say that after the \u0026lsquo;fun\u0026rsquo; I\u0026rsquo;ve had debugging pipelines in certain CI tools in the past.\nHere\u0026rsquo;s my complete workflow to change my blog to use GitHub Actions:\nChanged my old source and master branches to main for the code and gh-pages for the static content Installed cschleiden.vscode-github-actions GitHub Actions extension in VSCode which gives you a language engine and workflow visualisation. Added .github/workflows folder to the root of the repo. Created a new file called build-site.yml. Added the new code to build my Hugo site and publish it to a gh-pages branch. Pushed the code up to GitHub and watched the CI do its thing. Here is the full workflow file with some annotations for each section:\nname: site-deployment # Tell GH Actions to only run on a push to the \u0026#39;main\u0026#39; branch on: push: branches: - main jobs: # \u0026#39;deploy\u0026#39; is the arbitrary name of our job deploy: runs-on: ubuntu-18.04 steps: # This uses the Checkout action to grab our code including the theme submodule - name: Git Checkout uses: actions/checkout@v2 with: submodules: true fetch-depth: 0 # Installs v0.75.1 of Hugo using the Hugo Action - name: Install Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;0.75.1\u0026#39; # Run \u0026#39;hugo\u0026#39; to generate the static content - defaults to the ./public folder - name: Build Site run: hugo # This publishes our static site in ./public to the default gh-pages branch # GITHUB_TOKEN is an automatic token to allow authentication for Actions # We\u0026#39;ve also got a custom domain set up here - name: Deploy Site uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public cname: matthorgan.xyz As you can see from the above, this is a super simple way to get your site built and deployed on a commit. The build time for this is now ~30s which is comparable to AppVeyor but having no custom code makes it look much cleaner. I\u0026rsquo;m looking forward to getting into the weeds with GH Actions on some more complicated projects but initial impressions are very good!\n"
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/troubleshooting-terraform-kubernetes-helm/",
        "title": "Troubleshooting Terraform Kubernetes and Helm deployment",
        "section": "blog",
        "tags": ["terraform","helm","kubernetes","devops","automation"],
        "date" : "2019.09.23",
        "body": "I decided to rebuild my home lab Plex server using Kubernetes and a great Helm chart (https://github.com/munnerz/kube-plex) which dispatches transcode jobs as pods on the Kubernetes cluster. I wanted to do this in a fully automated fashion so that I could destroy and rebuild the whole infrastructure with one command thus saving me precious pennies and preventing any snowflake environments forming.\nI used Terraform to build the Azure Kubernetes Service (AKS) and all was going well until I tried integrating the Terraform Helm resources into my Terraform code. The AKS cluster was building absolutely fine but as soon as it got to the Helm resource, it immediately bombed with the following error:\nError: error installing: Post https://k8stest-aa211a06.hcp.ukwest.azmk8s.io:443/apis/extensions/v1beta1/namespaces/kube-system/deployments: dial tcp 92.242.132.15:443: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond. Reading the error at face value, it looked like the Helm resource couldn\u0026rsquo;t connect to my Kubernetes cluster on the hostname k8stest-aa211a06.hcp.ukwest.azmk8s.io. This was strange because using kubectl cluster-info showed the cluster up and running:\nKubernetes master is running at https://k8stest-bbf7a87b.hcp.ukwest.azmk8s.io:443 CoreDNS is running at https://k8stest-bbf7a87b.hcp.ukwest.azmk8s.io:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy kubernetes-dashboard is running at https://k8stest-bbf7a87b.hcp.ukwest.azmk8s.io:443/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy Metrics-server is running at https://k8stest-bbf7a87b.hcp.ukwest.azmk8s.io:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy If you\u0026rsquo;re eagle eyed, you might have already spotted the issue here but at this point, I was still scratching my head. As Terraform is idempotent, I ran a terraform apply --auto-approve to try everything again. As the Cluster was already up and working, Terraform would realise this and only try and deploy the Helm resource again:\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\nErr, right. So all I\u0026rsquo;ve done is reapply the same Terraform configuration and now my Helm chart has successfully deployed to my AKS cluster? At this point I thought that perhaps my local kubectl environment hadn\u0026rsquo;t been set up to point to the new AKS cluster I was created. I added a local_exec resource to initialise the environment with my AKS credentials just before I kick off the Helm resource:\nresource \u0026#34;null_resource\u0026#34; \u0026#34;initialise_kubectl\u0026#34; { provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;az aks get-credentials --resource-group ${var.resource_group_name} --name ${var.cluster_name} --overwrite-existing\u0026#34; } depends_on = [azurerm_kubernetes_cluster.k8s, azurerm_public_ip.plex_publicip] } I destroyed my current Terraform environment and reran it and unfortunately got the same error:\nError: error installing: Post https://k8stest-bbf7a87b.hcp.ukwest.azmk8s.io:443/apis/extensions/v1beta1/namespaces/kube-system/deployments: dial tcp 92.242.132.15:443: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond. Right, let\u0026rsquo;s just check whether this hostname is resolvable:\nPinging k8stest-bbf7a87b.hcp.ukwest.azmk8s.io [92.242.132.15] with 32 bytes of data: Request timed out. Request timed out. Request timed out. Nope. However, we know the Cluster has built successfully so this FQDN should definitely have an IP associated with it. Let\u0026rsquo;s jump over to PowerShell and check what the Cluster looks like:\nGet-AzAks ProvisioningState : Succeeded DnsPrefix : k8stest Fqdn : k8stest-134c5320.hcp.ukwest.azmk8s.io KubernetesVersion : 1.13.10 AgentPoolProfiles : {agentpool} LinuxProfile : Microsoft.Azure.Commands.Aks.Models.PSContainerServiceLinuxProfile ServicePrincipalProfile : Microsoft.Azure.Commands.Aks.Models.PSContainerServiceServicePrincipalProfile Id : /subscriptions/ed31d49b-a568-490a-8ee2-0cbaec65bc9b/resourcegroups/azure-k8stest/providers/Microsoft.ContainerService/managedClusters/k8stest Name : k8stest Type : Microsoft.ContainerService/ManagedClusters Location : ukwest Tags : {[Environment, Development]} Hold up a minute, that FQDN isn\u0026rsquo;t the same one that Helm was trying to connect on. As everything in our code is dynamically generated, where is it getting this hostname from? I was scratching my head for a while and then saw that the hostname it\u0026rsquo;s trying to connect to is actually the hostname of the previous build. So the hostname must be cached somewhere and it\u0026rsquo;s picking up that one instead of the correct one. Looking at the config file in the .kube folder in my home directory, it was showing the correct hostname but within the .kube folder there\u0026rsquo;s a cache folder that contains sub-folders of all of your previous builds so I started wondering whether it was picking up a cached hostname.\nJust in case this was a weird caching issue, I decided to delete my .kube folder because the AKS Terraform resource would recreate it as part of the build anyway but when I tried to run my terraform apply again, I got the following error:\nError: CreateFile C:\\Users\\matth\\.kube\\config: The system cannot find the path specified. With the above error, Terraform didn\u0026rsquo;t even try to apply the configuration. The next step was to comment out the Helm section of the Terraform and low and behold; no compilation error.\nAt this point it became pretty obvious what was going on. The Helm resource grabs your Kubernetes Cluster information from the config file in the .kube folder at the very start of your Terraform build. I was trying to dynamically create my Kubernetes config during the build with that local-exec command I mentioned earlier. Helm was basically picking up whatever was already in my Kubernetes config file and trying to connect to that. It just so happened that I\u0026rsquo;d ran previous AKS builds and with each dynamic hostname being so similar, took a bit of digging around to get to the root cause.\nSolution The quickest solution to this is to separate out the AKS configuration from the Helm configuration and ensure the Kubernetes config file is up to date with my latest AKS details before I run the Helm resource.\nHowever, a much better way to sort this is to configure the Terraform Helm Provider to accept the correct Kubernetes settings. The azurerm_kubernetes_cluster resource has a bunch of useful outputs that we can use to dynamically initialise Helm after our fresh Kubernetes cluster has been built:\nprovider \u0026#34;helm\u0026#34; { kubernetes { host = azurerm_kubernetes_cluster.aks_cluster.host client_certificate = base64decode(azurerm_kubernetes_cluster.aks_cluster.client_certificate) client_key = base64decode(azurerm_kubernetes_cluster.aks_cluster.client_key) cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks_cluster.cluster_ca_certificate) } } In the above code, we\u0026rsquo;re telling Helm to use the values from our brand new Kubernetes cluster resource as opposed to just loading the default Kubernetes config file which it was doing before we had the Helm provider configured.\nOne thing to note with providers is that as of Terraform v0.12, you can\u0026rsquo;t use a depends_on to force it to wait for a specific resource. Luckily for us, Terraform works best with implicit dependencies - as we\u0026rsquo;ve specified values that have come directly from our Kubernetes resource, the Helm provider will wait until it\u0026rsquo;s complete until it initialises.\nHacky tip of the day If you ever need an explicit dependency for your Provider, I stumbled across this cool little work-around solution somebody suggested on GitHub: https://github.com/hashicorp/terraform/issues/2430#issuecomment-524547219\n"
    }
]
