[
    {
        "ref": "https://matthorgan.xyz/blog/powershell-handling-native-applications/",
        "title": "Handling Native Applications in PowerShell 7.3+ with $PSNativeCommandErrorActionPreference",
        "section": "blog",
        "tags": ["powershell","automation","devops"],
        "date" : "2023.02.28",
        "body": "I\u0026rsquo;ve previously written a blog post about how to handle Azure CLI errors in PowerShell. The general pattern involves redirecting any error streams to the correct place, and checking for a known exit code. There\u0026rsquo;s an exciting new feature in PowerShell from version 7.3 onwards that makes capturing errors for native applications like the Azure CLI much simpler. Enter: $PSNativeCommandErrorActionPreference\nPSNativeCommandErrorActionPreference is a preference variable that when set to true, allows for native commands to be handled in a more \u0026lsquo;PowerShell\u0026rsquo; way.\nTo enable it, you just need to set $PSNativeCommandErrorActionPreference = $true somewhere in your script, and then you can use a conventional try/catch for your error handling. Check out this before/after example:\nBefore Assign variable to output Redirect the error stream and assign to variable $err to ensure any error(s) get captured If the $LASTEXITCODE isn\u0026rsquo;t a success, throw the contents of the $err variable $err = $($podList = kubectl get pods) 2\u0026gt;\u0026amp;1 if ($LASTEXITCODE -ne 0) { throw $err } After Set the $PSNativeCommandUseErrorActionPreference to true Set $ErrorActionPreference to Stop Handle the error in a traditional try/catch and it\u0026rsquo;ll throw if there\u0026rsquo;s a non-zero exit code (Note you may still want to redirect the error stream depending on the command) $PSNativeCommandUseErrorActionPreference = $true $ErrorActionPreference = \u0026#39;Stop\u0026#39; try { $podList = kubectl get pods } catch { throw } "
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/powershell-unexpected-character-error/",
        "title": "PowerShell Unexpected Character Error",
        "section": "blog",
        "tags": ["powershell","automation","azure","ci/cd","devops"],
        "date" : "2023.01.05",
        "body": "Over the years, I\u0026rsquo;ve had a few different Unexpected character encountered errors in PowerShell and for the longest time, they were always a real pain to troubleshoot and looked like a bug. Once you know the issue though, it\u0026rsquo;s a fairly simple one to solve. The latest one I\u0026rsquo;ve come across is running the following command from our CI/CD Linux build container to a Windows 2012 machine hosted in Azure:\nInvoke-AzVMRunCommand -ResourceGroupName $VMResourceGroupName -Name $VMName -CommandId 'RunPowerShellScript' -ScriptPath \u0026quot;tests/gather-vm-info.ps1\u0026quot;\nThe above command is using the Azure VM run command to run the contents of tests/gather-vm-info.ps1 on the remote Windows 2012 VM. It should be noted, I was running the exact same command on Windows 2016, and Windows 2019 VMs without any issues. The error I was seeing was the following:\nJsonReaderException: Unexpected character encountered while parsing value: W. Path \u0026#39;\u0026#39;, line 0, position 0. ArgumentException: Conversion from JSON failed with error: Unexpected character encountered while parsing value: W. Path \u0026#39;\u0026#39;, line 0, position 0. at \u0026lt;ScriptBlock\u0026gt;, /builds/golden-images/tests/gather-vm-info.ps1:49 Initially looking at this error, it felt \u0026lsquo;buggy\u0026rsquo; considering there was no \u0026lsquo;W\u0026rsquo; in my output when I ran the tests/gather-vm-info.ps1 code individually on the VM itself. I assumed it must be an issue with the Invoke-AzVMRunCommand but actually, when you look closer at the output, it\u0026rsquo;s easy to see what the issue is.\nValue[0] : Code : ComponentStatus/StdOut/succeeded Level : Info DisplayStatus : Provisioning succeeded Message : WARNING: Ping to \u0026lt;redacted URL\u0026gt; failed -- Status: TimedOut { \u0026#34;Output1\u0026#34;: true, \u0026#34;Output2\u0026#34;: true, \u0026#34;Output3\u0026#34;: true, \u0026#34;Output4\u0026#34;: true, \u0026#34;Output5\u0026#34;: false, \u0026#34;Output6\u0026#34;: true, \u0026#34;Output7\u0026#34;: true, \u0026#34;Output8\u0026#34;: false, \u0026#34;Output9\u0026#34;: false } As you can see, on investigating the Value.Message property of the object returned from Invoke-AzVMRunCommand, we\u0026rsquo;ve got a warning message at the very start of the output before our custom JSON. Suddenly, the W character in the initial error makes sense. In our scenario, Value.Message returns all output including any warning messages and therefore whilst our JSON is being returned correctly, we need to ensure that any warning messages are properly handled or suppressed to ensure that our output only returns JSON ready to be converted.\nSo, the next time you see a weird unexpected parsing error, have a look at what the starting character is. If it\u0026rsquo;s a W, there\u0026rsquo;s a good chance you could be trying to parse something that has captured one of the PowerShell output streams along with your expected output.\n"
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/powershell-type-value-gotchas/",
        "title": "Handling null values in PowerShell and the sneaky gotcha that might catch you out",
        "section": "blog",
        "tags": ["powershell","automation","ci/cd","devops"],
        "date" : "2022.09.15",
        "body": "In this blog post, I\u0026rsquo;ll be covering a fun little null value related \u0026lsquo;gotcha\u0026rsquo; that has caught me out over the years, and the explanation for why it happens. Let\u0026rsquo;s jump right in with how we handle null values and the issues I\u0026rsquo;ve come across in the past.\nHandling Null Values In PowerShell, there are several ways to check for a null value in a variable. One of the standard ways looks like this:\n$MyVariable = $null if ($null -eq $MyVariable) { \u0026#34;The variable is null\u0026#34; } The variable is null Another option is to use the static .NET String class and call the IsNullOrEmpty method like this:\n$MyVariable = $null if ([String]::IsNullOrEmpty($MyVariable)) { \u0026#34;The variable is null\u0026#34; } The variable is null Or, if you want to handle whitespace too, you could use the IsNullOrWhiteSpace method:\n# Whitespace variable that we want to check for $MyVariable = \u0026#39; \u0026#39; if ([String]::IsNullOrWhiteSpace($MyVariable)) { \u0026#34;The variable is null or is whitespace\u0026#34; } The variable is null or is whitespace The Null String \u0026lsquo;Gotcha\u0026rsquo; A lot of the time, I found myself opting for the IsNullOrWhiteSpace option because I\u0026rsquo;d previously been caught out by $null -eq $MyVariable not giving me the result I was expecting. I never fully understood why until recently when I was caught out once again.\nA significant portion of my PowerShell scripts and functions are used in CI/CD pipelines, where environment variables inherited from the CI/CD tool, such as a hostname or username, can be present. Instead of explicitly passing every variable into my scripts/functions, I use a pattern like this:\nfunction New-ExampleFunction { [CmdletBinding()] param ( [Parameter(Mandatory = $false)] [ValidateNotNullOrEmpty()] [String]$MyVariable = $env:VAR_FROM_CI_ENVIRONMENT ) Write-Host $MyVariable } As seen in the above function, the environment variable VAR_FROM_CI_ENVIRONMENT is available within the pipeline of our CI/CD tool. Therefore, we don\u0026rsquo;t need to provide a value for $MyVariable explicitly in this case. This approach offers the flexibility of passing a value to the parameter if desired, while defaulting to the known environment variable otherwise.\nThe issue however, is that outside the scope of our CI/CD environment, what happens if somebody doesn\u0026rsquo;t pass in a value for $MyVariable and they don\u0026rsquo;t have the $env:VAR_FROM_CI_ENVIRONMENT environment variable configured? We need to add an additional bit of validation in our function to ensure that $MyVariable contains a value if one isn\u0026rsquo;t passed in (The [ValidateNotNullOrEmpty()] is only validating values being passed in to $MyVariable, and not something we set as a default value).\nAdding some validation to the above function to check for null can create a nasty little gotcha and this is the main topic for this blog post. I added a traditional $null -eq $MyVariable check to ensure if $MyVariable was null and expected that it would throw a message and stop the function (Assuming nothing had been passed in OR the $env:VAR_FROM_CI_ENVIRONMENT environment variable hadn\u0026rsquo;t been set):\n# This does not produce the expected outcome function New-ExampleFunction { [CmdletBinding()] param ( [Parameter(Mandatory = $false)] [ValidateNotNullOrEmpty()] [String]$MyVariable = $env:VAR_FROM_CI_ENVIRONMENT ) if ($null -eq $MyVariable) { throw \u0026#39;The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please pass in a value or check the env var $env:VAR_FROM_CI_ENVIRONMENT is set\u0026#39; } Write-Host $MyVariable } When you run the above function without passing in a value for $MyVariable, and ensuring the VAR_FROM_CI_ENVIRONMENT env var is not set, nothing happens and we don\u0026rsquo;t see the error message we\u0026rsquo;re expecting.\nWell that\u0026rsquo;s not what I was expecting?! Checking that the env var is definitely not set by running $null -eq $env:VAR_FROM_CI_ENVIRONMENT comes back as True so it definitely IS null. Why isn\u0026rsquo;t PowerShell recognising it as null?\nIf we change how we\u0026rsquo;re checking for null to use the static string class method, look what happens:\n# This DOES produce the expected outcome function New-ExampleFunction { [CmdletBinding()] param ( [Parameter(Mandatory = $false)] [ValidateNotNullOrEmpty()] [String]$MyVariable = $env:VAR_FROM_CI_ENVIRONMENT ) if ([String]::IsNullOrEmpty($MyVariable)) { throw \u0026#39;The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please pass in a value or check the env var $env:VAR_FROM_CI_ENVIRONMENT is set\u0026#39; } Write-Host $MyVariable } Exception: Line | 14 | throw \u0026#39;The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please … | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ | The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please pass in a value or check the env var $env:VAR_FROM_CI_ENVIRONMENT is set This is now working as expected. But why? In the past I used to just change the $null -eq $MyVariable syntax to [String]::IsNullOrWhiteSpace(), wrongly assuming that the variable must have been whitespace but as you can see from the above working code, we\u0026rsquo;re using [String]::IsNullOrEmpty() so that theory has been proved to be incorrect.\nLook what happens when we remove the [String] casting from the function and we go back to using our $null -eq $MyVariable syntax:\nfunction New-ExampleFunction { [CmdletBinding()] param ( [Parameter(Mandatory = $false)] [ValidateNotNullOrEmpty()] # Note the [String] has now been removed $MyVariable = $env:VAR_FROM_CI_ENVIRONMENT ) if ($null -eq $MyVariable) { throw \u0026#39;The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please pass in a value or check the env var $env:VAR_FROM_CI_ENVIRONMENT is set\u0026#39; } Write-Host $MyVariable } We now see the error message that we were hoping to see originally (and that we saw with the [String]::IsNullOrEmpty() method):\nException: Line | 14 | throw \u0026#39;The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please … | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ | The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please pass in a value or check the env var $env:VAR_FROM_CI_ENVIRONMENT is set Conclusion Alrighty then. So it seems that casting the parameter MyVariable to a string is causing our original null comparison to fail. The reason for this is that when we add the [String] to our parameter $MyVariable and we don\u0026rsquo;t pass in a value, it takes the $env:VAR_FROM_CI_ENVIRONMENT and converts it into an empty string. When you strongly type a value type in PowerShell, it will convert $null into whatever the default value is for the type:\n[int]$number = $null $number 0 [bool]$boolean = $null $boolean False [string]$string = $null $string -eq \u0026#39;\u0026#39; True This explains the initial failure of our $null -eq $MyVariable example. PowerShell is converting the empty $MyVariable into the default value for a string and an empty string is not truly null. Using the string class method [String]::IsNullOrEmpty() ensures proper handling of an empty string.\nWhen $MyVariable wasn\u0026rsquo;t strongly typed to any specific type, the $null -eq $MyVariable comparison worked because, at that point, PowerShell didn\u0026rsquo;t know the type and therefore didn\u0026rsquo;t convert it to a default value of any specific type\nAnd there we have it. I can finally sleep a little easier now that I have a clear picture of the potential type-related issues when evaluating $null and why they occur.\n"
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/azcli-error-handling-in-powershell/",
        "title": "Azure CLI Error Handling in PowerShell",
        "section": "blog",
        "tags": ["powershell","automation","azcli","ci/cd","devops"],
        "date" : "2021.09.16",
        "body": "The Azure CLI (azcli) is an extremely useful tool in interacting with the Azure platform. I\u0026rsquo;ve been favouring it over the Azure PowerShell modules recently due to many of the commands being idempotent (E.g. not having to check if something already exists before trying to create can be a nice time saver). One of the downsides to using the azcli in PowerShell scripts however, is that you can\u0026rsquo;t handle errors like you would with a typical PowerShell cmdlet.\nBefore we get into the code, it\u0026rsquo;s worth saying that to filter data with the Azure CLI, you\u0026rsquo;ve got two options. Option one is to use the JMESPath query parameter e.g. in the following example az vm show --resource-group QueryDemo --name TestVM --query \u0026quot;osProfile.linuxConfiguration.ssh.publicKeys\u0026quot;. The default output from the Azure CLI is JSON and so this query is targeting a nested property publicKeys which in JSON would look something like \u0026quot;osProfile:\u0026quot; { \u0026quot;linuxConfiguration\u0026quot;: {\u0026quot;ssh\u0026quot;: { \u0026quot;publicKeys\u0026quot;: [{ \u0026quot;someData\u0026quot;: \u0026quot;someDataHere\u0026quot; }]} } }. The option I prefer however, is to use PowerShell\u0026rsquo;s ConvertFrom-Json cmdlet which brings us into our warm and cosy PowerShell world. So the same filter would be: (az vm show --resource-group QueryDemo --name TestVM | ConvertFrom-Json).osProfile.linuxConfiguration.ssh.publicKeys. I\u0026rsquo;ll be adding the ConvertFrom-Json cmdlet to the examples as this is more representitive of how I\u0026rsquo;d actually use the azcli.\nConsider the below examples with an App Registration that doesn\u0026rsquo;t exist:\ntry { $appReg = Get-AzureADApplication -ObjectId \u0026#39;NotARealObjectId\u0026#39; -ErrorAction \u0026#39;Stop\u0026#39; } catch { Write-Error \u0026#34;Uh oh, we\u0026#39;ve got an error here...\u0026#34; throw } try { $appReg = az ad app show --id \u0026#39;NotARealObjectId\u0026#39; | ConvertFrom-Json } catch { Write-Error \u0026#34;Uh oh, we\u0026#39;ve got an error here...\u0026#34; throw } In the first example, we get a lovely handled error and a custom message but in the azcli example, nothing gets thrown because the azcli is an external tool and PowerShell doesn\u0026rsquo;t natively know what to do with it.\nWe can improve things by making use of the $LASTEXITCODE which will give you the exit code of the last ran command.\n$appReg = az ad sp show --id \u0026#39;NotARealObjectId\u0026#39; | ConvertFrom-Json if ($LASTEXITCODE -ne 0) { Write-Error \u0026#34;Uh oh, we\u0026#39;ve got an error here...\u0026#34; -ErrorAction \u0026#39;Stop\u0026#39; } The above example will display our custom error message and halt the script if we get an exit code that isn\u0026rsquo;t 0. You\u0026rsquo;ll also find that the azcli spits out its own error but at the moment this isn\u0026rsquo;t information that we can capture within the constraints of our own error handling - it\u0026rsquo;ll just show the error as soon as the command isn\u0026rsquo;t successful.\nSo how do we capture the azcli error within our own error handling? We can utilise a bit of stream redirection and a subexpression to achieve a better result:\n$errOutput = $($appReg = \u0026amp; {az ad sp show --id \u0026#39;NotARealObjectId\u0026#39; | ConvertFrom-Json}) 2\u0026gt;\u0026amp;1 if ($errOutput) { Write-Error \u0026#34;Uh oh, we\u0026#39;ve got an error here...\u0026#34; -ErrorAction \u0026#39;Continue\u0026#39; throw $errOutput } In this final example, we\u0026rsquo;re executing the azcli command using the ampersand operator and capturing the variable output in the $appReg variable like before. However, this time we\u0026rsquo;re using a subexpression and redirecting the error stream of that subexpression to the success stream and capturing it in a variable called $errOutput. Now that we\u0026rsquo;ve got the error in a variable, we can display it and handle it however we like.\nOne thing to note is make sure you\u0026rsquo;re aware of what output the azcli command you\u0026rsquo;re using gives you. Some azcli commands send useful non-error commands to the error stream and therefore redirecting it like we\u0026rsquo;ve done above will trigger an error. If you wanted to, you could technically use a hybrid of the above two methods to ensure this doesn\u0026rsquo;t trip you up:\n$errOutput = $($appReg = \u0026amp; {az ad sp show --id \u0026#39;NotARealObjectId\u0026#39; | ConvertFrom-Json}) 2\u0026gt;\u0026amp;1 if ($LASTEXITCODE -ne 0) { Write-Error \u0026#34;Uh oh, we\u0026#39;ve got an error here...\u0026#34; -ErrorAction \u0026#39;Continue\u0026#39; throw $errOutput } "
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/chart-testing-gitlabci-error/",
        "title": "Error using chart-testing in GitLab Pipeline",
        "section": "blog",
        "tags": ["gitlab","automation","helm","chart-testing","ci/cd","devops"],
        "date" : "2021.01.12",
        "body": "We\u0026rsquo;ve recently been moving over from Jenkins to GitLab and as part of this, I was creating a validation pipeline for our Helm charts using chart-testing. When I tried to use the tool for some basic linting using ct lint, I kept getting the error:\nError: Error linting charts: Error identifying charts to process: Error running process: exit status 128.\nHowever, when I ran ct lint --all, everything seemed to work OK and the charts were analysed as expected. Looking further into the documentation for chart-testing, if you run --all, it\u0026rsquo;ll ignore any git functionality and just analyse the charts. Without that parameter, it\u0026rsquo;ll compare with what is already in source control and only analyse the charts that have differences.\nBy default, Gitlab CI does a shallow clone which means there is no git history for the tool to look at. Unfortunately, the exit status 128 that you get back from the tool didn\u0026rsquo;t help pinpoint it but once you disable the shallow clone within GitLab CI, chart-testing will now have the git information it needs to only analyse the charts that have changed.\nFYI, to disable shallow clone in GitLab CI, add an environment variable called GIT_DEPTH and set it to 0:\nvariables: GIT_DEPTH: 0 "
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/troubleshooting-terraform-kubernetes-helm/",
        "title": "Troubleshooting Terraform Kubernetes and Helm deployment",
        "section": "blog",
        "tags": ["terraform","helm","kubernetes","devops","automation"],
        "date" : "2019.09.23",
        "body": "I decided to rebuild my home lab Plex server using Kubernetes and a great Helm chart (https://github.com/munnerz/kube-plex) which dispatches transcode jobs as pods on the Kubernetes cluster. I wanted to do this in a fully automated fashion so that I could destroy and rebuild the whole infrastructure with one command thus saving me precious pennies and preventing any snowflake environments forming.\nI used Terraform to build the Azure Kubernetes Service (AKS) and all was going well until I tried integrating the Terraform Helm resources into my Terraform code. The AKS cluster was building absolutely fine but as soon as it got to the Helm resource, it immediately bombed with the following error:\nError: error installing: Post https://k8stest-aa211a06.hcp.ukwest.azmk8s.io:443/apis/extensions/v1beta1/namespaces/kube-system/deployments: dial tcp 92.242.132.15:443: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond. Reading the error at face value, it looked like the Helm resource couldn\u0026rsquo;t connect to my Kubernetes cluster on the hostname k8stest-aa211a06.hcp.ukwest.azmk8s.io. This was strange because using kubectl cluster-info showed the cluster up and running:\nKubernetes master is running at https://k8stest-bbf7a87b.hcp.ukwest.azmk8s.io:443 CoreDNS is running at https://k8stest-bbf7a87b.hcp.ukwest.azmk8s.io:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy kubernetes-dashboard is running at https://k8stest-bbf7a87b.hcp.ukwest.azmk8s.io:443/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy Metrics-server is running at https://k8stest-bbf7a87b.hcp.ukwest.azmk8s.io:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy If you\u0026rsquo;re eagle eyed, you might have already spotted the issue here but at this point, I was still scratching my head. As Terraform is idempotent, I ran a terraform apply --auto-approve to try everything again. As the Cluster was already up and working, Terraform would realise this and only try and deploy the Helm resource again:\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\nErr, right. So all I\u0026rsquo;ve done is reapply the same Terraform configuration and now my Helm chart has successfully deployed to my AKS cluster? At this point I thought that perhaps my local kubectl environment hadn\u0026rsquo;t been set up to point to the new AKS cluster I was created. I added a local_exec resource to initialise the environment with my AKS credentials just before I kick off the Helm resource:\nresource \u0026#34;null_resource\u0026#34; \u0026#34;initialise_kubectl\u0026#34; { provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;az aks get-credentials --resource-group ${var.resource_group_name} --name ${var.cluster_name} --overwrite-existing\u0026#34; } depends_on = [azurerm_kubernetes_cluster.k8s, azurerm_public_ip.plex_publicip] } I destroyed my current Terraform environment and reran it and unfortunately got the same error:\nError: error installing: Post https://k8stest-bbf7a87b.hcp.ukwest.azmk8s.io:443/apis/extensions/v1beta1/namespaces/kube-system/deployments: dial tcp 92.242.132.15:443: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond. Right, let\u0026rsquo;s just check whether this hostname is resolvable:\nPinging k8stest-bbf7a87b.hcp.ukwest.azmk8s.io [92.242.132.15] with 32 bytes of data: Request timed out. Request timed out. Request timed out. Nope. However, we know the Cluster has built successfully so this FQDN should definitely have an IP associated with it. Let\u0026rsquo;s jump over to PowerShell and check what the Cluster looks like:\nGet-AzAks ProvisioningState : Succeeded DnsPrefix : k8stest Fqdn : k8stest-134c5320.hcp.ukwest.azmk8s.io KubernetesVersion : 1.13.10 AgentPoolProfiles : {agentpool} LinuxProfile : Microsoft.Azure.Commands.Aks.Models.PSContainerServiceLinuxProfile ServicePrincipalProfile : Microsoft.Azure.Commands.Aks.Models.PSContainerServiceServicePrincipalProfile Id : /subscriptions/ed31d49b-a568-490a-8ee2-0cbaec65bc9b/resourcegroups/azure-k8stest/providers/Microsoft.ContainerService/managedClusters/k8stest Name : k8stest Type : Microsoft.ContainerService/ManagedClusters Location : ukwest Tags : {[Environment, Development]} Hold up a minute, that FQDN isn\u0026rsquo;t the same one that Helm was trying to connect on. As everything in our code is dynamically generated, where is it getting this hostname from? I was scratching my head for a while and then saw that the hostname it\u0026rsquo;s trying to connect to is actually the hostname of the previous build. So the hostname must be cached somewhere and it\u0026rsquo;s picking up that one instead of the correct one. Looking at the config file in the .kube folder in my home directory, it was showing the correct hostname but within the .kube folder there\u0026rsquo;s a cache folder that contains sub-folders of all of your previous builds so I started wondering whether it was picking up a cached hostname.\nJust in case this was a weird caching issue, I decided to delete my .kube folder because the AKS Terraform resource would recreate it as part of the build anyway but when I tried to run my terraform apply again, I got the following error:\nError: CreateFile C:\\Users\\matth\\.kube\\config: The system cannot find the path specified. With the above error, Terraform didn\u0026rsquo;t even try to apply the configuration. The next step was to comment out the Helm section of the Terraform and low and behold; no compilation error.\nAt this point it became pretty obvious what was going on. The Helm resource grabs your Kubernetes Cluster information from the config file in the .kube folder at the very start of your Terraform build. I was trying to dynamically create my Kubernetes config during the build with that local-exec command I mentioned earlier. Helm was basically picking up whatever was already in my Kubernetes config file and trying to connect to that. It just so happened that I\u0026rsquo;d ran previous AKS builds and with each dynamic hostname being so similar, took a bit of digging around to get to the root cause.\nSolution The quickest solution to this is to separate out the AKS configuration from the Helm configuration and ensure the Kubernetes config file is up to date with my latest AKS details before I run the Helm resource.\nHowever, a much better way to sort this is to configure the Terraform Helm Provider to accept the correct Kubernetes settings. The azurerm_kubernetes_cluster resource has a bunch of useful outputs that we can use to dynamically initialise Helm after our fresh Kubernetes cluster has been built:\nprovider \u0026#34;helm\u0026#34; { kubernetes { host = azurerm_kubernetes_cluster.aks_cluster.host client_certificate = base64decode(azurerm_kubernetes_cluster.aks_cluster.client_certificate) client_key = base64decode(azurerm_kubernetes_cluster.aks_cluster.client_key) cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks_cluster.cluster_ca_certificate) } } In the above code, we\u0026rsquo;re telling Helm to use the values from our brand new Kubernetes cluster resource as opposed to just loading the default Kubernetes config file which it was doing before we had the Helm provider configured.\nOne thing to note with providers is that as of Terraform v0.12, you can\u0026rsquo;t use a depends_on to force it to wait for a specific resource. Luckily for us, Terraform works best with implicit dependencies - as we\u0026rsquo;ve specified values that have come directly from our Kubernetes resource, the Helm provider will wait until it\u0026rsquo;s complete until it initialises.\nHacky tip of the day If you ever need an explicit dependency for your Provider, I stumbled across this cool little work-around solution somebody suggested on GitHub: https://github.com/hashicorp/terraform/issues/2430#issuecomment-524547219\n"
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/decrypt-secure-string/",
        "title": "Decrypting PowerShell Secure Strings",
        "section": "blog",
        "tags": ["powershell","automation"],
        "date" : "2019.08.15",
        "body": "Ever found yourself needing to decrypt a secure string knowing that there are a couple of static methods you need to use but can never remember what they are? After a quick Google, you\u0026rsquo;ll probably stumble upon something similar to this:\n$BSTR = [System.Runtime.InteropServices.Marshal]::SecureStringToBSTR($SecurePassword) $UnsecurePassword = [System.Runtime.InteropServices.Marshal]::PtrToStringAuto($BSTR) Whilst this works fine, it\u0026rsquo;s way easier to remember (Tab complete is your friend) that a PSCredential object has the ability to show the password by using GetNetworkCredential().Password. So, you can construct a PSCredential object with arbitrary username (I used a whitespace) and access the secure string like so:\n$UnsecurePassword = [PSCredential]::New(\u0026#39; \u0026#39;, $SecurePassword).GetNetworkCredential().Password "
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/register-datawarehouse/",
        "title": "Register-SCDWSource Cmdlet hidden parameter",
        "section": "blog",
        "tags": ["powershell","automation","service manager","system center"],
        "date" : "2018.08.01",
        "body": "Myself and a colleague were recently given the task of end-to-end automation of Sytem Center Service Manager using PowerShell DSC. This needed to include prerequisites, installation of the product, and any post configuration.\nIf any of you are familiar with Service Manager, you\u0026rsquo;ll know that one of the steps is to register Service Manager to the data warehouse. Luckily, there\u0026rsquo;s a PowerShell cmdlet for that which should make the task a breeze: Register-SCDWSource\nLooking at the documentation, I\u0026rsquo;ve modified Microsoft\u0026rsquo;s example to use my credentials and VM names to register the datawarehouse with Service Manager:\n$creds = New-Object -TypeName \u0026#39;PSCredential\u0026#39; -ArgumentList (\u0026#39;lab\\Administrator\u0026#39;, (ConvertTo-SecureString -String \u0026#39;MyLabPassword\u0026#39; -AsPlainText -Force)) Register-SCDWSource -ComputerName \u0026#39;scsmdw1\u0026#39; -SourceComputerName \u0026#39;scsmms1\u0026#39; -DataSourceTypeName \u0026#39;ServiceManager\u0026#39; -Credential $creds When you run this command, a credential request pop-up box appears which would imply that the $creds variable hasn\u0026rsquo;t worked hence the prompt. Looking back at the example online, there\u0026rsquo;s nothing obvious missed and the $creds variable contains a credential object as you\u0026rsquo;d expect.\nUpon cancelling the prompt pop-up box, things started becoming clear:\nI should have paid more attention to the initial message behind the pop-up box - the -Credential parameter and $creds variable were working exactly as expected BUT the Register-SCDWSource cmdlet actually required the SourceCredential parameter too. I tried to use the auto-complete functionality for the SourceCredential parameter but nothing was coming up (and nothing was mentioned about this parameter in the documentation) which filled me with 0% confidence. However, I ended up trying it and to my shock, the cmdlet worked a treat:\n$creds = New-Object -TypeName \u0026#39;PSCredential\u0026#39; -ArgumentList (\u0026#39;lab\\Administrator\u0026#39;, (ConvertTo-SecureString -String \u0026#39;MyLabPassword\u0026#39; -AsPlainText -Force)) Register-SCDWSource -ComputerName \u0026#39;scsmdw1\u0026#39; -SourceComputerName \u0026#39;scsmms1\u0026#39; -DataSourceTypeName \u0026#39;ServiceManager\u0026#39; -Credential $creds -SourceCredential $creds A great lesson here is that we\u0026rsquo;re all human and even Microsoft are going to make mistakes with their documentation and cmdlets. Hopefully the feedback will ensure that they update the documentation and if not then, hopefully this post helps!\n"
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/vagrant-up-not-working/",
        "title": "Error when running 'vagrant up'",
        "section": "blog",
        "tags": ["powershell","automation","vagrant","virtualbox"],
        "date" : "2018.01.16",
        "body": "I\u0026rsquo;ve been using vagrant to build up labs for my work recently and it\u0026rsquo;s an awesome product. One small downside is the sheer amount of space that the new VMs were taking up on my laptop SSD (I\u0026rsquo;d been testing linked clones and hadn\u0026rsquo;t cleaned up after myself). As my whole lab build was in code, I felt rather pleased with myself that I could just bin the whole environment including any VMs and then just spin it up again afterwards. So, off I went to the VirtualBox VM directory (\u0026quot;$env:userprofile\\VirtualBox VMs\u0026quot;). When I looked at the total size of the directory, it was over 100GB in space and because being heavy handed works so well in I.T (Hmm\u0026hellip;), I deleted all of the VMs in the folder.\nRight, time to \u0026lsquo;vagarnt up\u0026rsquo; my lab and sit back and watch the infrastructure as code goodness:\nAH! Perhaps my heavy handed approach probably wasn\u0026rsquo;t the best idea. It seems like there might be some sort of lock on the files Vagrant is trying to use. Wait, there shouldn\u0026rsquo;t be any files needed because they\u0026rsquo;ve all been deleted, right? Looking in VirtualBox shows that it still thinks the VMs exist but are inaccessible (I forgot to grab a screenshot, sorry!). To fix this, I deleted the inaccessible VMs from within VirtualBox and ran a \u0026lsquo;vagrant up\u0026rsquo; again and it imported a fresh box with no errors.\nAs I\u0026rsquo;m using linked clones for my environment, Vagrant must have been looking for the initial clone that my lab was supposed to be using (The one that I\u0026rsquo;d heavy handedly deleted). As VirtualBox still thought it had the clone in existence, Vagrant was spitting out errors when trying to build the lab.\n"
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/powershell-dsc-resourcekey-empty/",
        "title": "PowerShell DSC ResourceKey Empty",
        "section": "blog",
        "tags": ["powershell","automation","dsc","sql"],
        "date" : "2018.01.04",
        "body": "I\u0026rsquo;m absolutely loving PowerShell DSC at the moment and we\u0026rsquo;re using it heavily for the automation and configuration of various products. One of the most recent tasks was to create a fully parameterised SQL build function for reusability across the company. All was going swimmingly until I ran into the following error when attempting to run my freshly parameterised DSC function:\nAfter a little bit of digging, I stumbled across a great blog post by Jacob Benson which had some nice troubleshooting steps (Link here). This blog pointed me in the right direction - the key property of the DSC resource always has to be present. Well, weirdly, all of my parameters were being correctly passed through because I\u0026rsquo;d verified that in a verbose message:\nI had a quick look in the MOF file for the DSC resource I was using (SqlSetup in SqlServerDsc)and noted that the key was \u0026lsquo;InstanceName\u0026rsquo;. I then changed the InstanceName parameter to a hardcoded value and re-ran the configuration\u0026hellip; BINGO! It worked absolutely fine which meant there must have been something wrong with my InstanceName parameter.\nA DSC configuration will automatically have three default parameters when it is created - InstanceName, OutputPath, and ConfigurationData. After rolling back to a fresh snapshot in my lab, I decided to change the name of the parameter I was using for InstanceName because I\u0026rsquo;d actually just called it $InstanceName for simplicity. I renamed it to $SqlInstanceName and re-ran the configuration and SQL installed like a charm.\nI\u0026rsquo;ve never had to use the default parameter \u0026lsquo;InstanceName\u0026rsquo; for any of my configurations so it took me a while to realise what was going wrong. However, when you think that any variables with the same names as default parameter names are going to be overwritten with the default parameter values, it makes perfect sense. The original error wasn\u0026rsquo;t misleading at all - \u0026lsquo;ResourceKey\u0026rsquo; WAS an empty string and next time I\u0026rsquo;ll be more careful with my function parameter names to ensure they don\u0026rsquo;t clash with any DSC configuration default parameters.\n"
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/invoke-command-grouppolicy-problem/",
        "title": "Invoke-Command Group Policy Gotcha (Whoops!)",
        "section": "blog",
        "tags": ["powershell","automation","invoke-command","gpupdate /force"],
        "date" : "2017.07.09",
        "body": "A task that I\u0026rsquo;ve been working on recently is the automation of VM builds and post configuration using PowerShell and the PowerCLI module (Unfortunately we don\u0026rsquo;t have a configuration management tool to make our lives easier at the moment).\nPart of the post-build configuration is to run a bunch of commands on the newly built VMs. I could have used PowerCLI\u0026rsquo;s Invoke-VMScript command to achieve this however as all of the VMs were joining the domain and tools on some of the VM templates were out of date, I used PowerShell\u0026rsquo;s Invoke-Command. This was going swimmingly until I ran into an issue\u0026hellip;\nThe final requirement of the initial post configuration was to force refresh Group Policy on the machine. Easy right?\nInvoke-Command -ComputerName $VmName -ScriptBlock {gpupdate /force} Upon running this command, I was faced with absolutely nothing being returned - the prompt was just hanging. Eh?! So, what happens if I just run a gpupdate without the force switch?\nRight, so that works perfectly. Hmm, maybe the sheer number of policies that need to apply is slowing things down? What happens if we try a VM that already has the right policies?\nOK, so this seems to imply that the issue is purely with the initial application of the policies. The next step is to RDP onto the VM and try the command locally to see what gets returned:\nBingo! The Group Policy update was working fine but required a reboot. Of course Invoke-Command isn\u0026rsquo;t going to return anything if the remote command it\u0026rsquo;s running is sat waiting for user input.\nResolution Armed with the above information, the solution was rather simple - gpupdate has a /boot flag which reboots the VM if required. So all that we needed to get Invoke-Command working correctly was:\nInvoke-Command -ComputerName $VmName -ScriptBlock {gpupdate /force /boot} The take-away here is to always consider that a particular command might not behave the same way you\u0026rsquo;ve seen it behave on other machines in the past. As a side note, if there wasn\u0026rsquo;t a /boot switch then you could probably spoof the user input required by doing something like:\nInvoke-Command -ComputerName $VmName -ScriptBlock {echo \u0026#34;Y\u0026#34; | gpupdate /force} "
    }
]
