[
    {
        "ref": "https://matthorgan.xyz/blog/powershell-handling-native-applications/",
        "title": "Handling Native Applications in PowerShell 7.3+ with $PSNativeCommandErrorActionPreference",
        "section": "blog",
        "tags": ["powershell","automation","devops"],
        "date" : "2023.02.28",
        "body": "I\u0026rsquo;ve previously written a blog post about how to handle Azure CLI errors in PowerShell. The general pattern involves redirecting any error streams to the correct place, and checking for a known exit code. There\u0026rsquo;s an exciting new feature in PowerShell from version 7.3 onwards that makes capturing errors for native applications like the Azure CLI much simpler. Enter: $PSNativeCommandErrorActionPreference\nPSNativeCommandErrorActionPreference is a preference variable that when set to true, allows for native commands to be handled in a more \u0026lsquo;PowerShell\u0026rsquo; way.\nTo enable it, you just need to set $PSNativeCommandErrorActionPreference = $true somewhere in your script, and then you can use a conventional try/catch for your error handling. Check out this before/after example:\nBefore Assign variable to output Redirect the error stream and assign to variable $err to ensure any error(s) get captured If the $LASTEXITCODE isn\u0026rsquo;t a success, throw the contents of the $err variable $err = $($podList = kubectl get pods) 2\u0026gt;\u0026amp;1 if ($LASTEXITCODE -ne 0) { throw $err } After Set the $PSNativeCommandUseErrorActionPreference to true Set $ErrorActionPreference to Stop Handle the error in a traditional try/catch and it\u0026rsquo;ll throw if there\u0026rsquo;s a non-zero exit code (Note you may still want to redirect the error stream depending on the command) $PSNativeCommandUseErrorActionPreference = $true $ErrorActionPreference = \u0026#39;Stop\u0026#39; try { $podList = kubectl get pods } catch { throw } "
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/powershell-unexpected-character-error/",
        "title": "PowerShell Unexpected Character Error",
        "section": "blog",
        "tags": ["powershell","automation","azure","ci/cd","devops"],
        "date" : "2023.01.05",
        "body": "Over the years, I\u0026rsquo;ve had a few different Unexpected character encountered errors in PowerShell and for the longest time, they were always a real pain to troubleshoot and looked like a bug. Once you know the issue though, it\u0026rsquo;s a fairly simple one to solve. The latest one I\u0026rsquo;ve come across is running the following command from our CI/CD Linux build container to a Windows 2012 machine hosted in Azure:\nInvoke-AzVMRunCommand -ResourceGroupName $VMResourceGroupName -Name $VMName -CommandId 'RunPowerShellScript' -ScriptPath \u0026quot;tests/gather-vm-info.ps1\u0026quot;\nThe above command is using the Azure VM run command to run the contents of tests/gather-vm-info.ps1 on the remote Windows 2012 VM. It should be noted, I was running the exact same command on Windows 2016, and Windows 2019 VMs without any issues. The error I was seeing was the following:\nJsonReaderException: Unexpected character encountered while parsing value: W. Path \u0026#39;\u0026#39;, line 0, position 0. ArgumentException: Conversion from JSON failed with error: Unexpected character encountered while parsing value: W. Path \u0026#39;\u0026#39;, line 0, position 0. at \u0026lt;ScriptBlock\u0026gt;, /builds/golden-images/tests/gather-vm-info.ps1:49 Initially looking at this error, it felt \u0026lsquo;buggy\u0026rsquo; considering there was no \u0026lsquo;W\u0026rsquo; in my output when I ran the tests/gather-vm-info.ps1 code individually on the VM itself. I assumed it must be an issue with the Invoke-AzVMRunCommand but actually, when you look closer at the output, it\u0026rsquo;s easy to see what the issue is.\nValue[0] : Code : ComponentStatus/StdOut/succeeded Level : Info DisplayStatus : Provisioning succeeded Message : WARNING: Ping to \u0026lt;redacted URL\u0026gt; failed -- Status: TimedOut { \u0026#34;Output1\u0026#34;: true, \u0026#34;Output2\u0026#34;: true, \u0026#34;Output3\u0026#34;: true, \u0026#34;Output4\u0026#34;: true, \u0026#34;Output5\u0026#34;: false, \u0026#34;Output6\u0026#34;: true, \u0026#34;Output7\u0026#34;: true, \u0026#34;Output8\u0026#34;: false, \u0026#34;Output9\u0026#34;: false } As you can see, on investigating the Value.Message property of the object returned from Invoke-AzVMRunCommand, we\u0026rsquo;ve got a warning message at the very start of the output before our custom JSON. Suddenly, the W character in the initial error makes sense. In our scenario, Value.Message returns all output including any warning messages and therefore whilst our JSON is being returned correctly, we need to ensure that any warning messages are properly handled or suppressed to ensure that our output only returns JSON ready to be converted.\nSo, the next time you see a weird unexpected parsing error, have a look at what the starting character is. If it\u0026rsquo;s a W, there\u0026rsquo;s a good chance you could be trying to parse something that has captured one of the PowerShell output streams along with your expected output.\n"
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/powershell-type-value-gotchas/",
        "title": "Handling null values in PowerShell and the sneaky gotcha that might catch you out",
        "section": "blog",
        "tags": ["powershell","automation","ci/cd","devops"],
        "date" : "2022.09.15",
        "body": "In this blog post, I\u0026rsquo;ll be covering a fun little null value related \u0026lsquo;gotcha\u0026rsquo; that has caught me out over the years, and the explanation for why it happens. Let\u0026rsquo;s jump right in with how we handle null values and the issues I\u0026rsquo;ve come across in the past.\nHandling Null Values In PowerShell, there are several ways to check for a null value in a variable. One of the standard ways looks like this:\n$MyVariable = $null if ($null -eq $MyVariable) { \u0026#34;The variable is null\u0026#34; } The variable is null Another option is to use the static .NET String class and call the IsNullOrEmpty method like this:\n$MyVariable = $null if ([String]::IsNullOrEmpty($MyVariable)) { \u0026#34;The variable is null\u0026#34; } The variable is null Or, if you want to handle whitespace too, you could use the IsNullOrWhiteSpace method:\n# Whitespace variable that we want to check for $MyVariable = \u0026#39; \u0026#39; if ([String]::IsNullOrWhiteSpace($MyVariable)) { \u0026#34;The variable is null or is whitespace\u0026#34; } The variable is null or is whitespace The Null String \u0026lsquo;Gotcha\u0026rsquo; A lot of the time, I found myself opting for the IsNullOrWhiteSpace option because I\u0026rsquo;d previously been caught out by $null -eq $MyVariable not giving me the result I was expecting. I never fully understood why until recently when I was caught out once again.\nA significant portion of my PowerShell scripts and functions are used in CI/CD pipelines, where environment variables inherited from the CI/CD tool, such as a hostname or username, can be present. Instead of explicitly passing every variable into my scripts/functions, I use a pattern like this:\nfunction New-ExampleFunction { [CmdletBinding()] param ( [Parameter(Mandatory = $false)] [ValidateNotNullOrEmpty()] [String]$MyVariable = $env:VAR_FROM_CI_ENVIRONMENT ) Write-Host $MyVariable } As seen in the above function, the environment variable VAR_FROM_CI_ENVIRONMENT is available within the pipeline of our CI/CD tool. Therefore, we don\u0026rsquo;t need to provide a value for $MyVariable explicitly in this case. This approach offers the flexibility of passing a value to the parameter if desired, while defaulting to the known environment variable otherwise.\nThe issue however, is that outside the scope of our CI/CD environment, what happens if somebody doesn\u0026rsquo;t pass in a value for $MyVariable and they don\u0026rsquo;t have the $env:VAR_FROM_CI_ENVIRONMENT environment variable configured? We need to add an additional bit of validation in our function to ensure that $MyVariable contains a value if one isn\u0026rsquo;t passed in (The [ValidateNotNullOrEmpty()] is only validating values being passed in to $MyVariable, and not something we set as a default value).\nAdding some validation to the above function to check for null can create a nasty little gotcha and this is the main topic for this blog post. I added a traditional $null -eq $MyVariable check to ensure if $MyVariable was null and expected that it would throw a message and stop the function (Assuming nothing had been passed in OR the $env:VAR_FROM_CI_ENVIRONMENT environment variable hadn\u0026rsquo;t been set):\n# This does not produce the expected outcome function New-ExampleFunction { [CmdletBinding()] param ( [Parameter(Mandatory = $false)] [ValidateNotNullOrEmpty()] [String]$MyVariable = $env:VAR_FROM_CI_ENVIRONMENT ) if ($null -eq $MyVariable) { throw \u0026#39;The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please pass in a value or check the env var $env:VAR_FROM_CI_ENVIRONMENT is set\u0026#39; } Write-Host $MyVariable } When you run the above function without passing in a value for $MyVariable, and ensuring the VAR_FROM_CI_ENVIRONMENT env var is not set, nothing happens and we don\u0026rsquo;t see the error message we\u0026rsquo;re expecting.\nWell that\u0026rsquo;s not what I was expecting?! Checking that the env var is definitely not set by running $null -eq $env:VAR_FROM_CI_ENVIRONMENT comes back as True so it definitely IS null. Why isn\u0026rsquo;t PowerShell recognising it as null?\nIf we change how we\u0026rsquo;re checking for null to use the static string class method, look what happens:\n# This DOES produce the expected outcome function New-ExampleFunction { [CmdletBinding()] param ( [Parameter(Mandatory = $false)] [ValidateNotNullOrEmpty()] [String]$MyVariable = $env:VAR_FROM_CI_ENVIRONMENT ) if ([String]::IsNullOrEmpty($MyVariable)) { throw \u0026#39;The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please pass in a value or check the env var $env:VAR_FROM_CI_ENVIRONMENT is set\u0026#39; } Write-Host $MyVariable } Exception: Line | 14 | throw \u0026#39;The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please … | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ | The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please pass in a value or check the env var $env:VAR_FROM_CI_ENVIRONMENT is set This is now working as expected. But why? In the past I used to just change the $null -eq $MyVariable syntax to [String]::IsNullOrWhiteSpace(), wrongly assuming that the variable must have been whitespace but as you can see from the above working code, we\u0026rsquo;re using [String]::IsNullOrEmpty() so that theory has been proved to be incorrect.\nLook what happens when we remove the [String] casting from the function and we go back to using our $null -eq $MyVariable syntax:\nfunction New-ExampleFunction { [CmdletBinding()] param ( [Parameter(Mandatory = $false)] [ValidateNotNullOrEmpty()] # Note the [String] has now been removed $MyVariable = $env:VAR_FROM_CI_ENVIRONMENT ) if ($null -eq $MyVariable) { throw \u0026#39;The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please pass in a value or check the env var $env:VAR_FROM_CI_ENVIRONMENT is set\u0026#39; } Write-Host $MyVariable } We now see the error message that we were hoping to see originally (and that we saw with the [String]::IsNullOrEmpty() method):\nException: Line | 14 | throw \u0026#39;The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please … | ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ | The value for parameter \u0026#34;MyVariable\u0026#34; is blank. Please pass in a value or check the env var $env:VAR_FROM_CI_ENVIRONMENT is set Conclusion Alrighty then. So it seems that casting the parameter MyVariable to a string is causing our original null comparison to fail. The reason for this is that when we add the [String] to our parameter $MyVariable and we don\u0026rsquo;t pass in a value, it takes the $env:VAR_FROM_CI_ENVIRONMENT and converts it into an empty string. When you strongly type a value type in PowerShell, it will convert $null into whatever the default value is for the type:\n[int]$number = $null $number 0 [bool]$boolean = $null $boolean False [string]$string = $null $string -eq \u0026#39;\u0026#39; True This explains the initial failure of our $null -eq $MyVariable example. PowerShell is converting the empty $MyVariable into the default value for a string and an empty string is not truly null. Using the string class method [String]::IsNullOrEmpty() ensures proper handling of an empty string.\nWhen $MyVariable wasn\u0026rsquo;t strongly typed to any specific type, the $null -eq $MyVariable comparison worked because, at that point, PowerShell didn\u0026rsquo;t know the type and therefore didn\u0026rsquo;t convert it to a default value of any specific type\nAnd there we have it. I can finally sleep a little easier now that I have a clear picture of the potential type-related issues when evaluating $null and why they occur.\n"
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/azcli-error-handling-in-powershell/",
        "title": "Azure CLI Error Handling in PowerShell",
        "section": "blog",
        "tags": ["powershell","automation","azcli","ci/cd","devops"],
        "date" : "2021.09.16",
        "body": "The Azure CLI (azcli) is an extremely useful tool in interacting with the Azure platform. I\u0026rsquo;ve been favouring it over the Azure PowerShell modules recently due to many of the commands being idempotent (E.g. not having to check if something already exists before trying to create can be a nice time saver). One of the downsides to using the azcli in PowerShell scripts however, is that you can\u0026rsquo;t handle errors like you would with a typical PowerShell cmdlet.\nBefore we get into the code, it\u0026rsquo;s worth saying that to filter data with the Azure CLI, you\u0026rsquo;ve got two options. Option one is to use the JMESPath query parameter e.g. in the following example az vm show --resource-group QueryDemo --name TestVM --query \u0026quot;osProfile.linuxConfiguration.ssh.publicKeys\u0026quot;. The default output from the Azure CLI is JSON and so this query is targeting a nested property publicKeys which in JSON would look something like \u0026quot;osProfile:\u0026quot; { \u0026quot;linuxConfiguration\u0026quot;: {\u0026quot;ssh\u0026quot;: { \u0026quot;publicKeys\u0026quot;: [{ \u0026quot;someData\u0026quot;: \u0026quot;someDataHere\u0026quot; }]} } }. The option I prefer however, is to use PowerShell\u0026rsquo;s ConvertFrom-Json cmdlet which brings us into our warm and cosy PowerShell world. So the same filter would be: (az vm show --resource-group QueryDemo --name TestVM | ConvertFrom-Json).osProfile.linuxConfiguration.ssh.publicKeys. I\u0026rsquo;ll be adding the ConvertFrom-Json cmdlet to the examples as this is more representitive of how I\u0026rsquo;d actually use the azcli.\nConsider the below examples with an App Registration that doesn\u0026rsquo;t exist:\ntry { $appReg = Get-AzureADApplication -ObjectId \u0026#39;NotARealObjectId\u0026#39; -ErrorAction \u0026#39;Stop\u0026#39; } catch { Write-Error \u0026#34;Uh oh, we\u0026#39;ve got an error here...\u0026#34; throw } try { $appReg = az ad app show --id \u0026#39;NotARealObjectId\u0026#39; | ConvertFrom-Json } catch { Write-Error \u0026#34;Uh oh, we\u0026#39;ve got an error here...\u0026#34; throw } In the first example, we get a lovely handled error and a custom message but in the azcli example, nothing gets thrown because the azcli is an external tool and PowerShell doesn\u0026rsquo;t natively know what to do with it.\nWe can improve things by making use of the $LASTEXITCODE which will give you the exit code of the last ran command.\n$appReg = az ad sp show --id \u0026#39;NotARealObjectId\u0026#39; | ConvertFrom-Json if ($LASTEXITCODE -ne 0) { Write-Error \u0026#34;Uh oh, we\u0026#39;ve got an error here...\u0026#34; -ErrorAction \u0026#39;Stop\u0026#39; } The above example will display our custom error message and halt the script if we get an exit code that isn\u0026rsquo;t 0. You\u0026rsquo;ll also find that the azcli spits out its own error but at the moment this isn\u0026rsquo;t information that we can capture within the constraints of our own error handling - it\u0026rsquo;ll just show the error as soon as the command isn\u0026rsquo;t successful.\nSo how do we capture the azcli error within our own error handling? We can utilise a bit of stream redirection and a subexpression to achieve a better result:\n$errOutput = $($appReg = \u0026amp; {az ad sp show --id \u0026#39;NotARealObjectId\u0026#39; | ConvertFrom-Json}) 2\u0026gt;\u0026amp;1 if ($errOutput) { Write-Error \u0026#34;Uh oh, we\u0026#39;ve got an error here...\u0026#34; -ErrorAction \u0026#39;Continue\u0026#39; throw $errOutput } In this final example, we\u0026rsquo;re executing the azcli command using the ampersand operator and capturing the variable output in the $appReg variable like before. However, this time we\u0026rsquo;re using a subexpression and redirecting the error stream of that subexpression to the success stream and capturing it in a variable called $errOutput. Now that we\u0026rsquo;ve got the error in a variable, we can display it and handle it however we like.\nOne thing to note is make sure you\u0026rsquo;re aware of what output the azcli command you\u0026rsquo;re using gives you. Some azcli commands send useful non-error commands to the error stream and therefore redirecting it like we\u0026rsquo;ve done above will trigger an error. If you wanted to, you could technically use a hybrid of the above two methods to ensure this doesn\u0026rsquo;t trip you up:\n$errOutput = $($appReg = \u0026amp; {az ad sp show --id \u0026#39;NotARealObjectId\u0026#39; | ConvertFrom-Json}) 2\u0026gt;\u0026amp;1 if ($LASTEXITCODE -ne 0) { Write-Error \u0026#34;Uh oh, we\u0026#39;ve got an error here...\u0026#34; -ErrorAction \u0026#39;Continue\u0026#39; throw $errOutput } "
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/chart-testing-gitlabci-error/",
        "title": "Error using chart-testing in GitLab Pipeline",
        "section": "blog",
        "tags": ["gitlab","automation","helm","chart-testing","ci/cd","devops"],
        "date" : "2021.01.12",
        "body": "We\u0026rsquo;ve recently been moving over from Jenkins to GitLab and as part of this, I was creating a validation pipeline for our Helm charts using chart-testing. When I tried to use the tool for some basic linting using ct lint, I kept getting the error:\nError: Error linting charts: Error identifying charts to process: Error running process: exit status 128.\nHowever, when I ran ct lint --all, everything seemed to work OK and the charts were analysed as expected. Looking further into the documentation for chart-testing, if you run --all, it\u0026rsquo;ll ignore any git functionality and just analyse the charts. Without that parameter, it\u0026rsquo;ll compare with what is already in source control and only analyse the charts that have differences.\nBy default, Gitlab CI does a shallow clone which means there is no git history for the tool to look at. Unfortunately, the exit status 128 that you get back from the tool didn\u0026rsquo;t help pinpoint it but once you disable the shallow clone within GitLab CI, chart-testing will now have the git information it needs to only analyse the charts that have changed.\nFYI, to disable shallow clone in GitLab CI, add an environment variable called GIT_DEPTH and set it to 0:\nvariables: GIT_DEPTH: 0 "
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/appveyor-to-github-actions/",
        "title": "Moving from AppVeyor to GitHub Actions",
        "section": "blog",
        "tags": ["appveyor","github","actions","devops","ci/cd"],
        "date" : "2020.11.26",
        "body": "It\u0026rsquo;s been over three and a half years since I first created my blog with Hugo and AppVeyor. Since then, I\u0026rsquo;ve been fortunate (or unfortunate depending on the tool!) to use lots of other CI/CD tools such as GitLab CI, Jenkins, Bamboo, Azure DevOps. GitHub Actions is one tool that I hadn\u0026rsquo;t used yet and it\u0026rsquo;s been on my radar for a while so why not take the opportunity to update the blog and the CI pipeline?\nOne of the main wins for me with GitHub Actions is the integration with GitHub. Having your code and your CI pipeline in one familiar GUI makes for a really nice experience. There\u0026rsquo;s also a great VSCode plugin I found called cschleiden.vscode-github-actions which provides a really nice view of all your pipeline builds (or workflows as they\u0026rsquo;re called in GitHub) like this:\nAnother great benefit of GitHub Actions is the marketplace that contains thousands of actions that can help automate your workflow.\nThe first thing I was going to do before I got started with GitHub Actions was to move my old inline code from the AppVeyor build file and into its own script. Over the years, I\u0026rsquo;ve realised that pipelines can get wildly out of hand if you don\u0026rsquo;t abstract as much code away from the pipeline syntax as possible. This makes your pipelines way easier to read and understand and allows for them to be easily portable into other CI tools if required.\nAfter some initial testing with the Windows 2019 runner, I decided to swap over to Linux as the builds went from around 30s in AppVeyor to a whole 3m30 in GitHub Actions.\nBefore I started to refactor my code into a little bash script, I thought I\u0026rsquo;d have a quick look at what community Actions were available to potentially simplify my workflow and lo and behold, it couldn\u0026rsquo;t have been simpler.\nThere\u0026rsquo;s a Hugo Action available from here which installs Hugo at the version you specify. On top of that, there\u0026rsquo;s a GitHub Pages Action which allows you to deploy your static content to your gh-pages branch or whichever branch your GitHub Pages is setup to point to.\nThis therefore meant that I didn\u0026rsquo;t actually need to use any scripts or any custom code at all and my workflow YAML file is wonderfully simple. My code is almost exactly the same as one of the examples in the Hugo Actions repo - everything just worked - what a joy it is to say that after the \u0026lsquo;fun\u0026rsquo; I\u0026rsquo;ve had debugging pipelines in certain CI tools in the past.\nHere\u0026rsquo;s my complete workflow to change my blog to use GitHub Actions:\nChanged my old source and master branches to main for the code and gh-pages for the static content Installed cschleiden.vscode-github-actions GitHub Actions extension in VSCode which gives you a language engine and workflow visualisation. Added .github/workflows folder to the root of the repo. Created a new file called build-site.yml. Added the new code to build my Hugo site and publish it to a gh-pages branch. Pushed the code up to GitHub and watched the CI do its thing. Here is the full workflow file with some annotations for each section:\nname: site-deployment # Tell GH Actions to only run on a push to the \u0026#39;main\u0026#39; branch on: push: branches: - main jobs: # \u0026#39;deploy\u0026#39; is the arbitrary name of our job deploy: runs-on: ubuntu-18.04 steps: # This uses the Checkout action to grab our code including the theme submodule - name: Git Checkout uses: actions/checkout@v2 with: submodules: true fetch-depth: 0 # Installs v0.75.1 of Hugo using the Hugo Action - name: Install Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;0.75.1\u0026#39; # Run \u0026#39;hugo\u0026#39; to generate the static content - defaults to the ./public folder - name: Build Site run: hugo # This publishes our static site in ./public to the default gh-pages branch # GITHUB_TOKEN is an automatic token to allow authentication for Actions # We\u0026#39;ve also got a custom domain set up here - name: Deploy Site uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public cname: matthorgan.xyz As you can see from the above, this is a super simple way to get your site built and deployed on a commit. The build time for this is now ~30s which is comparable to AppVeyor but having no custom code makes it look much cleaner. I\u0026rsquo;m looking forward to getting into the weeds with GH Actions on some more complicated projects but initial impressions are very good!\n"
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/troubleshooting-terraform-kubernetes-helm/",
        "title": "Troubleshooting Terraform Kubernetes and Helm deployment",
        "section": "blog",
        "tags": ["terraform","helm","kubernetes","devops","automation"],
        "date" : "2019.09.23",
        "body": "I decided to rebuild my home lab Plex server using Kubernetes and a great Helm chart (https://github.com/munnerz/kube-plex) which dispatches transcode jobs as pods on the Kubernetes cluster. I wanted to do this in a fully automated fashion so that I could destroy and rebuild the whole infrastructure with one command thus saving me precious pennies and preventing any snowflake environments forming.\nI used Terraform to build the Azure Kubernetes Service (AKS) and all was going well until I tried integrating the Terraform Helm resources into my Terraform code. The AKS cluster was building absolutely fine but as soon as it got to the Helm resource, it immediately bombed with the following error:\nError: error installing: Post https://k8stest-aa211a06.hcp.ukwest.azmk8s.io:443/apis/extensions/v1beta1/namespaces/kube-system/deployments: dial tcp 92.242.132.15:443: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond. Reading the error at face value, it looked like the Helm resource couldn\u0026rsquo;t connect to my Kubernetes cluster on the hostname k8stest-aa211a06.hcp.ukwest.azmk8s.io. This was strange because using kubectl cluster-info showed the cluster up and running:\nKubernetes master is running at https://k8stest-bbf7a87b.hcp.ukwest.azmk8s.io:443 CoreDNS is running at https://k8stest-bbf7a87b.hcp.ukwest.azmk8s.io:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy kubernetes-dashboard is running at https://k8stest-bbf7a87b.hcp.ukwest.azmk8s.io:443/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy Metrics-server is running at https://k8stest-bbf7a87b.hcp.ukwest.azmk8s.io:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy If you\u0026rsquo;re eagle eyed, you might have already spotted the issue here but at this point, I was still scratching my head. As Terraform is idempotent, I ran a terraform apply --auto-approve to try everything again. As the Cluster was already up and working, Terraform would realise this and only try and deploy the Helm resource again:\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\nErr, right. So all I\u0026rsquo;ve done is reapply the same Terraform configuration and now my Helm chart has successfully deployed to my AKS cluster? At this point I thought that perhaps my local kubectl environment hadn\u0026rsquo;t been set up to point to the new AKS cluster I was created. I added a local_exec resource to initialise the environment with my AKS credentials just before I kick off the Helm resource:\nresource \u0026#34;null_resource\u0026#34; \u0026#34;initialise_kubectl\u0026#34; { provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;az aks get-credentials --resource-group ${var.resource_group_name} --name ${var.cluster_name} --overwrite-existing\u0026#34; } depends_on = [azurerm_kubernetes_cluster.k8s, azurerm_public_ip.plex_publicip] } I destroyed my current Terraform environment and reran it and unfortunately got the same error:\nError: error installing: Post https://k8stest-bbf7a87b.hcp.ukwest.azmk8s.io:443/apis/extensions/v1beta1/namespaces/kube-system/deployments: dial tcp 92.242.132.15:443: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond. Right, let\u0026rsquo;s just check whether this hostname is resolvable:\nPinging k8stest-bbf7a87b.hcp.ukwest.azmk8s.io [92.242.132.15] with 32 bytes of data: Request timed out. Request timed out. Request timed out. Nope. However, we know the Cluster has built successfully so this FQDN should definitely have an IP associated with it. Let\u0026rsquo;s jump over to PowerShell and check what the Cluster looks like:\nGet-AzAks ProvisioningState : Succeeded DnsPrefix : k8stest Fqdn : k8stest-134c5320.hcp.ukwest.azmk8s.io KubernetesVersion : 1.13.10 AgentPoolProfiles : {agentpool} LinuxProfile : Microsoft.Azure.Commands.Aks.Models.PSContainerServiceLinuxProfile ServicePrincipalProfile : Microsoft.Azure.Commands.Aks.Models.PSContainerServiceServicePrincipalProfile Id : /subscriptions/ed31d49b-a568-490a-8ee2-0cbaec65bc9b/resourcegroups/azure-k8stest/providers/Microsoft.ContainerService/managedClusters/k8stest Name : k8stest Type : Microsoft.ContainerService/ManagedClusters Location : ukwest Tags : {[Environment, Development]} Hold up a minute, that FQDN isn\u0026rsquo;t the same one that Helm was trying to connect on. As everything in our code is dynamically generated, where is it getting this hostname from? I was scratching my head for a while and then saw that the hostname it\u0026rsquo;s trying to connect to is actually the hostname of the previous build. So the hostname must be cached somewhere and it\u0026rsquo;s picking up that one instead of the correct one. Looking at the config file in the .kube folder in my home directory, it was showing the correct hostname but within the .kube folder there\u0026rsquo;s a cache folder that contains sub-folders of all of your previous builds so I started wondering whether it was picking up a cached hostname.\nJust in case this was a weird caching issue, I decided to delete my .kube folder because the AKS Terraform resource would recreate it as part of the build anyway but when I tried to run my terraform apply again, I got the following error:\nError: CreateFile C:\\Users\\matth\\.kube\\config: The system cannot find the path specified. With the above error, Terraform didn\u0026rsquo;t even try to apply the configuration. The next step was to comment out the Helm section of the Terraform and low and behold; no compilation error.\nAt this point it became pretty obvious what was going on. The Helm resource grabs your Kubernetes Cluster information from the config file in the .kube folder at the very start of your Terraform build. I was trying to dynamically create my Kubernetes config during the build with that local-exec command I mentioned earlier. Helm was basically picking up whatever was already in my Kubernetes config file and trying to connect to that. It just so happened that I\u0026rsquo;d ran previous AKS builds and with each dynamic hostname being so similar, took a bit of digging around to get to the root cause.\nSolution The quickest solution to this is to separate out the AKS configuration from the Helm configuration and ensure the Kubernetes config file is up to date with my latest AKS details before I run the Helm resource.\nHowever, a much better way to sort this is to configure the Terraform Helm Provider to accept the correct Kubernetes settings. The azurerm_kubernetes_cluster resource has a bunch of useful outputs that we can use to dynamically initialise Helm after our fresh Kubernetes cluster has been built:\nprovider \u0026#34;helm\u0026#34; { kubernetes { host = azurerm_kubernetes_cluster.aks_cluster.host client_certificate = base64decode(azurerm_kubernetes_cluster.aks_cluster.client_certificate) client_key = base64decode(azurerm_kubernetes_cluster.aks_cluster.client_key) cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks_cluster.cluster_ca_certificate) } } In the above code, we\u0026rsquo;re telling Helm to use the values from our brand new Kubernetes cluster resource as opposed to just loading the default Kubernetes config file which it was doing before we had the Helm provider configured.\nOne thing to note with providers is that as of Terraform v0.12, you can\u0026rsquo;t use a depends_on to force it to wait for a specific resource. Luckily for us, Terraform works best with implicit dependencies - as we\u0026rsquo;ve specified values that have come directly from our Kubernetes resource, the Helm provider will wait until it\u0026rsquo;s complete until it initialises.\nHacky tip of the day If you ever need an explicit dependency for your Provider, I stumbled across this cool little work-around solution somebody suggested on GitHub: https://github.com/hashicorp/terraform/issues/2430#issuecomment-524547219\n"
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/decrypt-secure-string/",
        "title": "Decrypting PowerShell Secure Strings",
        "section": "blog",
        "tags": ["powershell","automation"],
        "date" : "2019.08.15",
        "body": "Ever found yourself needing to decrypt a secure string knowing that there are a couple of static methods you need to use but can never remember what they are? After a quick Google, you\u0026rsquo;ll probably stumble upon something similar to this:\n$BSTR = [System.Runtime.InteropServices.Marshal]::SecureStringToBSTR($SecurePassword) $UnsecurePassword = [System.Runtime.InteropServices.Marshal]::PtrToStringAuto($BSTR) Whilst this works fine, it\u0026rsquo;s way easier to remember (Tab complete is your friend) that a PSCredential object has the ability to show the password by using GetNetworkCredential().Password. So, you can construct a PSCredential object with arbitrary username (I used a whitespace) and access the secure string like so:\n$UnsecurePassword = [PSCredential]::New(\u0026#39; \u0026#39;, $SecurePassword).GetNetworkCredential().Password "
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/register-datawarehouse/",
        "title": "Register-SCDWSource Cmdlet hidden parameter",
        "section": "blog",
        "tags": ["powershell","automation","service manager","system center"],
        "date" : "2018.08.01",
        "body": "Myself and a colleague were recently given the task of end-to-end automation of Sytem Center Service Manager using PowerShell DSC. This needed to include prerequisites, installation of the product, and any post configuration.\nIf any of you are familiar with Service Manager, you\u0026rsquo;ll know that one of the steps is to register Service Manager to the data warehouse. Luckily, there\u0026rsquo;s a PowerShell cmdlet for that which should make the task a breeze: Register-SCDWSource\nLooking at the documentation, I\u0026rsquo;ve modified Microsoft\u0026rsquo;s example to use my credentials and VM names to register the datawarehouse with Service Manager:\n$creds = New-Object -TypeName \u0026#39;PSCredential\u0026#39; -ArgumentList (\u0026#39;lab\\Administrator\u0026#39;, (ConvertTo-SecureString -String \u0026#39;MyLabPassword\u0026#39; -AsPlainText -Force)) Register-SCDWSource -ComputerName \u0026#39;scsmdw1\u0026#39; -SourceComputerName \u0026#39;scsmms1\u0026#39; -DataSourceTypeName \u0026#39;ServiceManager\u0026#39; -Credential $creds When you run this command, a credential request pop-up box appears which would imply that the $creds variable hasn\u0026rsquo;t worked hence the prompt. Looking back at the example online, there\u0026rsquo;s nothing obvious missed and the $creds variable contains a credential object as you\u0026rsquo;d expect.\nUpon cancelling the prompt pop-up box, things started becoming clear:\nI should have paid more attention to the initial message behind the pop-up box - the -Credential parameter and $creds variable were working exactly as expected BUT the Register-SCDWSource cmdlet actually required the SourceCredential parameter too. I tried to use the auto-complete functionality for the SourceCredential parameter but nothing was coming up (and nothing was mentioned about this parameter in the documentation) which filled me with 0% confidence. However, I ended up trying it and to my shock, the cmdlet worked a treat:\n$creds = New-Object -TypeName \u0026#39;PSCredential\u0026#39; -ArgumentList (\u0026#39;lab\\Administrator\u0026#39;, (ConvertTo-SecureString -String \u0026#39;MyLabPassword\u0026#39; -AsPlainText -Force)) Register-SCDWSource -ComputerName \u0026#39;scsmdw1\u0026#39; -SourceComputerName \u0026#39;scsmms1\u0026#39; -DataSourceTypeName \u0026#39;ServiceManager\u0026#39; -Credential $creds -SourceCredential $creds A great lesson here is that we\u0026rsquo;re all human and even Microsoft are going to make mistakes with their documentation and cmdlets. Hopefully the feedback will ensure that they update the documentation and if not then, hopefully this post helps!\n"
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/vagrant-up-not-working/",
        "title": "Error when running 'vagrant up'",
        "section": "blog",
        "tags": ["powershell","automation","vagrant","virtualbox"],
        "date" : "2018.01.16",
        "body": "I\u0026rsquo;ve been using vagrant to build up labs for my work recently and it\u0026rsquo;s an awesome product. One small downside is the sheer amount of space that the new VMs were taking up on my laptop SSD (I\u0026rsquo;d been testing linked clones and hadn\u0026rsquo;t cleaned up after myself). As my whole lab build was in code, I felt rather pleased with myself that I could just bin the whole environment including any VMs and then just spin it up again afterwards. So, off I went to the VirtualBox VM directory (\u0026quot;$env:userprofile\\VirtualBox VMs\u0026quot;). When I looked at the total size of the directory, it was over 100GB in space and because being heavy handed works so well in I.T (Hmm\u0026hellip;), I deleted all of the VMs in the folder.\nRight, time to \u0026lsquo;vagarnt up\u0026rsquo; my lab and sit back and watch the infrastructure as code goodness:\nAH! Perhaps my heavy handed approach probably wasn\u0026rsquo;t the best idea. It seems like there might be some sort of lock on the files Vagrant is trying to use. Wait, there shouldn\u0026rsquo;t be any files needed because they\u0026rsquo;ve all been deleted, right? Looking in VirtualBox shows that it still thinks the VMs exist but are inaccessible (I forgot to grab a screenshot, sorry!). To fix this, I deleted the inaccessible VMs from within VirtualBox and ran a \u0026lsquo;vagrant up\u0026rsquo; again and it imported a fresh box with no errors.\nAs I\u0026rsquo;m using linked clones for my environment, Vagrant must have been looking for the initial clone that my lab was supposed to be using (The one that I\u0026rsquo;d heavy handedly deleted). As VirtualBox still thought it had the clone in existence, Vagrant was spitting out errors when trying to build the lab.\n"
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/powershell-dsc-resourcekey-empty/",
        "title": "PowerShell DSC ResourceKey Empty",
        "section": "blog",
        "tags": ["powershell","automation","dsc","sql"],
        "date" : "2018.01.04",
        "body": "I\u0026rsquo;m absolutely loving PowerShell DSC at the moment and we\u0026rsquo;re using it heavily for the automation and configuration of various products. One of the most recent tasks was to create a fully parameterised SQL build function for reusability across the company. All was going swimmingly until I ran into the following error when attempting to run my freshly parameterised DSC function:\nAfter a little bit of digging, I stumbled across a great blog post by Jacob Benson which had some nice troubleshooting steps (Link here). This blog pointed me in the right direction - the key property of the DSC resource always has to be present. Well, weirdly, all of my parameters were being correctly passed through because I\u0026rsquo;d verified that in a verbose message:\nI had a quick look in the MOF file for the DSC resource I was using (SqlSetup in SqlServerDsc)and noted that the key was \u0026lsquo;InstanceName\u0026rsquo;. I then changed the InstanceName parameter to a hardcoded value and re-ran the configuration\u0026hellip; BINGO! It worked absolutely fine which meant there must have been something wrong with my InstanceName parameter.\nA DSC configuration will automatically have three default parameters when it is created - InstanceName, OutputPath, and ConfigurationData. After rolling back to a fresh snapshot in my lab, I decided to change the name of the parameter I was using for InstanceName because I\u0026rsquo;d actually just called it $InstanceName for simplicity. I renamed it to $SqlInstanceName and re-ran the configuration and SQL installed like a charm.\nI\u0026rsquo;ve never had to use the default parameter \u0026lsquo;InstanceName\u0026rsquo; for any of my configurations so it took me a while to realise what was going wrong. However, when you think that any variables with the same names as default parameter names are going to be overwritten with the default parameter values, it makes perfect sense. The original error wasn\u0026rsquo;t misleading at all - \u0026lsquo;ResourceKey\u0026rsquo; WAS an empty string and next time I\u0026rsquo;ll be more careful with my function parameter names to ensure they don\u0026rsquo;t clash with any DSC configuration default parameters.\n"
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/invoke-command-grouppolicy-problem/",
        "title": "Invoke-Command Group Policy Gotcha (Whoops!)",
        "section": "blog",
        "tags": ["powershell","automation","invoke-command","gpupdate /force"],
        "date" : "2017.07.09",
        "body": "A task that I\u0026rsquo;ve been working on recently is the automation of VM builds and post configuration using PowerShell and the PowerCLI module (Unfortunately we don\u0026rsquo;t have a configuration management tool to make our lives easier at the moment).\nPart of the post-build configuration is to run a bunch of commands on the newly built VMs. I could have used PowerCLI\u0026rsquo;s Invoke-VMScript command to achieve this however as all of the VMs were joining the domain and tools on some of the VM templates were out of date, I used PowerShell\u0026rsquo;s Invoke-Command. This was going swimmingly until I ran into an issue\u0026hellip;\nThe final requirement of the initial post configuration was to force refresh Group Policy on the machine. Easy right?\nInvoke-Command -ComputerName $VmName -ScriptBlock {gpupdate /force} Upon running this command, I was faced with absolutely nothing being returned - the prompt was just hanging. Eh?! So, what happens if I just run a gpupdate without the force switch?\nRight, so that works perfectly. Hmm, maybe the sheer number of policies that need to apply is slowing things down? What happens if we try a VM that already has the right policies?\nOK, so this seems to imply that the issue is purely with the initial application of the policies. The next step is to RDP onto the VM and try the command locally to see what gets returned:\nBingo! The Group Policy update was working fine but required a reboot. Of course Invoke-Command isn\u0026rsquo;t going to return anything if the remote command it\u0026rsquo;s running is sat waiting for user input.\nResolution Armed with the above information, the solution was rather simple - gpupdate has a /boot flag which reboots the VM if required. So all that we needed to get Invoke-Command working correctly was:\nInvoke-Command -ComputerName $VmName -ScriptBlock {gpupdate /force /boot} The take-away here is to always consider that a particular command might not behave the same way you\u0026rsquo;ve seen it behave on other machines in the past. As a side note, if there wasn\u0026rsquo;t a /boot switch then you could probably spoof the user input required by doing something like:\nInvoke-Command -ComputerName $VmName -ScriptBlock {echo \u0026#34;Y\u0026#34; | gpupdate /force} "
    }
,
    {
        "ref": "https://matthorgan.xyz/blog/hugo-and-appveyor-goodness/",
        "title": "Hugo \u0026 AppVeyor (Continuous Integration Blog Goodness)",
        "section": "blog",
        "tags": ["go","golang","hugo","development","powershell","winops","continuous integration","appveyor"],
        "date" : "2017.03.16",
        "body": "Recently, I\u0026rsquo;ve been thinking about how I can use DevOps tools and processes to improve the way I do things and this has led me to completely revamp my blog. My existing Wordpress blog has now been replaced with a shiny new blog using Hugo - a static website engine, hosted on Github Pages and automated using the continuous integration tool AppVeyor. In this article, I\u0026rsquo;m going to go through a step-by-step guide on how I set everything up.\nStep 1. Install Hugo \u0026amp; Build New Site First off, it\u0026rsquo;s worth mentioning that this step-by-step guide assumes you\u0026rsquo;re running a Windows environment and using the Windows package manager Chocolatey (If you don\u0026rsquo;t know about Chocolatey then you NEED to get this in your life right now. Check it out here).\nI won\u0026rsquo;t go into too much detail about Hugo because there\u0026rsquo;s some great stuff online already but essentially, Hugo renders all of your static content into HTML, CSS and JavaScript. This makes pages on the site really quick to load and easy to manage as you don\u0026rsquo;t rely on a traditional CMS to manage content. Install Hugo by running:\nchoco install hugo -y Next up, creating a new skeleton site structure is as simple as running:\nhugo new site mynewblog If you have a look in your newly created site folder, it\u0026rsquo;ll look something like this:\nNow that we have the basic layout, we need to add a theme. I ended up browsing the theme section on Hugo\u0026rsquo;s website to find one I liked. Once you\u0026rsquo;ve found a theme that you like, clone it into the themes folder of your newly created site.\nOn my local machine, I use posh-git which integrates Git nicely into PowerShell (Including tab completion). To install posh-git, simply run the following from PowerShell:\nchoco install poshgit -y Note: You may need to reload your PowerShell session to use the Git commands from PowerShell. With Git installed, clone the theme you like:\ncd themes git clone https://github.com/example/example-theme Usually, a theme will have an \u0026rsquo;exampleSite\u0026rsquo; directory which contains an example structure for the theme including an example config.toml configuration file:\nI copied these files over into the root of my new Hugo site and started tailoring to fit my needs. I won\u0026rsquo;t go into any further detail on that part - just play around with it and follow your nose. So, you\u0026rsquo;ve got a theme you\u0026rsquo;re happy with and you\u0026rsquo;ve tweaked the configuration to your liking? Hugo has a very convenient web server that listens on port 1313 so that you can view how your glorious website will look when built. To make use of this, run the command:\nhugo server The great thing about this is, every time you update any of your static files, the changes will instantly appear at http://localhost:1313 Right, now we\u0026rsquo;ve got our new Hugo site configured the way we want it, let\u0026rsquo;s set up our Github pages.\nStep 2. Setting up Github Pages to work with our new Hugo site If you haven\u0026rsquo;t already got a Github account, head over to GitHub and sign up for one. Once you\u0026rsquo;ve got your account set up, create a new repository and call it yourusername.github.io e.g. mine is matthorgan.github.io.\nFor this build, we\u0026rsquo;re going to be using two branches - \u0026lsquo;source\u0026rsquo; which will house all of the static content and \u0026lsquo;master\u0026rsquo; which will house the Hugo built content. Doing it this way allows us to have both the static and generated content completely version controlled. From PowerShell, browse to the root directory of your Hugo site and run the following commands to set up your local repository and push it to your Github repo:\ngit init git add --all git commit -m \u0026#34;Initial Hugo static file commit\u0026#34; git remote add origin https://github.com/yourusername/yourusername.github.io.git git push -u origin source Note: Replace \u0026lsquo;yourusername\u0026rsquo; with your actual Github username and repo name We\u0026rsquo;ve now got our static content pushed to a branch in our Github Pages repo called \u0026lsquo;source\u0026rsquo;. At this point, you can move on to the continuous integration with AppVeyor however just for completeness, I\u0026rsquo;ll continue to explain how you get your page online without a CI tool.\nType the following command from a PowerShell prompt in the root directory of your Hugo site to build the site:\nhugo If you have a look in the root directory, you\u0026rsquo;ll now see a \u0026lsquo;public\u0026rsquo; folder which has all of your built content e.g. HTML, JavaScript, CSS etc.\nChange directory into the public folder and type the following:\ngit checkout -b master git add --all git commit -m \u0026#34;Initial Hugo built site commit\u0026#34; git push -u origin master You should now have your website online and browsable at https://yourusername.github.io - take a moment to bask in its unfinished glory before we move on to the cool stuff!\nStep 3. Using AppVeyor to automate the Hugo build Having our newly generated website up and running already is cool but each time you add/modify any of the static content, the site will need to be rebuilt and pushed up to GitHub. That\u0026rsquo;s at least two buttons more than I want to have to press so we\u0026rsquo;re going to use AppVeyor to automate it for us. Head over to AppVeyor and sign up with GitHub if you haven\u0026rsquo;t already got an account. Once you\u0026rsquo;re logged in, you should be able to add your GitHub pages repo as a new project by clicking Projects \u0026ndash;\u0026gt; New Project and selecting your GitHub repo.\nAt this point, we have two options on how we want to configure the project. You can edit the settings of your project and configure everything within the AppVeyor UI or you can create an appveyor.yml file with the relevant settings and place it in the root of your GitHub repo. Here\u0026rsquo;s what you\u0026rsquo;ll see in the AppVeyor UI: You can see from the menu in the above screenshot that there are so many possibilities when it comes to the configuration of your project. A cool feature of the UI is that you can export to an appveyor.yml file which makes it really easy to understand what\u0026rsquo;s going on. Another great way to understand all of the possible settings within an appveyor.yml file is to check out this reference guide here.\nNote: The UI and appveyor.yml are mutually exclusive so you\u0026rsquo;ll have to pick one or the other.\nWith so many options available, it\u0026rsquo;s important to understand exactly what you want your build to accomplish. To stop me from going off on a tangent, I put together a basic flow for what I needed AppVeyor to do:\nClone the \u0026lsquo;source\u0026rsquo; branch into the AppVeyor build Set up environment by installing Hugo Build site with Hugo Clone \u0026lsquo;master\u0026rsquo; branch of GitHub Pages repo Copy all newly generated content into the newly cloned \u0026lsquo;master\u0026rsquo; directory Add and commit new files to \u0026lsquo;master\u0026rsquo; branch Push new commits back up to master If you have a look back at the screenshot of the AppVeyor settings, you can see an option to specify a default branch. I made sure to set this to my \u0026lsquo;source\u0026rsquo; branch which ensures AppVeyor clones the \u0026lsquo;source\u0026rsquo; branch into the AppVeyor build.\nBelow is the final appveyor.yml file that I ended up with (I\u0026rsquo;ll explain it in more detail shortly). I pushed this to my \u0026lsquo;source\u0026rsquo; branch which instructs AppVeyor to execute the build every time something new is pushed to the source branch e.g. a new blog post.\nversion: 1.0.{build} pull_requests: do_not_increment_build_number: true environment: source_dir: public git_name: Matt Horgan git_email: matt@matthorgan.xyz target_dir: temp target_branch: master repo: https://github.com/matthorgan/matthorgan.github.io.git access_token: secure: RcRrztcM59Im5xl40yelyNPtMZ8ydkEtFQ8nzen+4tcId4U8eT8yrTxTDOyUnJZu install: - cmd: cinst hugo build_script: - cmd: hugo # Add newly built site to the master branch on_success: - ps: Invoke-Expression \u0026#34;git config --global credential.helper store\u0026#34; 2\u0026gt;\u0026amp;1 - ps: Add-Content \u0026#34;$env:userprofile\\.git-credentials\u0026#34; \u0026#34;https://$($env:access_token):x-oauth-basic@github.com`n\u0026#34; - ps: $revision = Invoke-Expression \u0026#34;git rev-parse HEAD\u0026#34; 2\u0026gt;\u0026amp;1 - ps: New-Item -Path $env:target_dir -ItemType Directory - ps: cd .\\$env:target_dir - ps: Invoke-Expression \u0026#34;git clone --branch $env:target_branch $env:repo (Get-Location).Path\u0026#34; 2\u0026gt;\u0026amp;1 - ps: Copy-Item -Path ..\\$env:source_dir\\* -Destination . -Recurse -Force -Exclude @(\u0026#34;.git\u0026#34;,\u0026#34;appveyor.yml\u0026#34;) - ps: Invoke-Expression \u0026#34;git config --global user.name $env:git_name\u0026#34; 2\u0026gt;\u0026amp;1 - ps: Invoke-Expression \u0026#34;git config --global user.email $env:git_email\u0026#34; 2\u0026gt;\u0026amp;1 - ps: Invoke-Expression \u0026#34;git add --all\u0026#34; 2\u0026gt;\u0026amp;1 - ps: Invoke-Expression \u0026#34;git commit --allow-empty -m \u0026#39;Built from commit $revision\u0026#39;\u0026#34; 2\u0026gt;\u0026amp;1 - ps: Invoke-Expression \u0026#34;git push origin $env:target_branch\u0026#34; 2\u0026gt;\u0026amp;1 Some of the settings are self-explanatory but I\u0026rsquo;ll go through the different sections and explain what\u0026rsquo;s going on:\nenvironment: source_dir: public git_name: Matt Horgan git_email: matt@matthorgan.xyz target_dir: temp target_branch: master repo: https://github.com/matthorgan/matthorgan.github.io.git access_token: secure: RcRrztcM59Im5xl40yelyNPtMZ8ydkEtFQ8nzen+4tcId4U8eT8yrTxTDOyUnJZu In the above section, we set up our environment variables which get used further down in the appveyor.yml file. The crucial part of this section is the secure access token which must be created to give AppVeyor permissions to push to your repo. For a great guide on how to set this up, check out this article on the AppVeyor site.\nThe next interesting section of the AppVeyor YAML file is as shown:\ninstall: - cmd: cinst hugo build_script: - cmd: hugo The \u0026lsquo;cinst hugo\u0026rsquo; command tells AppVeyor to install Hugo using Chocolatey whilst the next line builds the site.\nFinally, the PowerShell section of the file:\non_success: - ps: Invoke-Expression \u0026#34;git config --global credential.helper store\u0026#34; 2\u0026gt;\u0026amp;1 - ps: Add-Content \u0026#34;$env:userprofile\\.git-credentials\u0026#34; \u0026#34;https://$($env:access_token):x-oauth-basic@github.com`n\u0026#34; - ps: $revision = Invoke-Expression \u0026#34;git rev-parse HEAD\u0026#34; 2\u0026gt;\u0026amp;1 - ps: New-Item -Path $env:target_dir -ItemType Directory - ps: cd .\\$env:target_dir - ps: Invoke-Expression \u0026#34;git clone --branch $env:target_branch $env:repo (Get-Location).Path\u0026#34; 2\u0026gt;\u0026amp;1 - ps: Copy-Item -Path ..\\$env:source_dir\\* -Destination . -Recurse -Force -Exclude @(\u0026#34;.git\u0026#34;,\u0026#34;appveyor.yml\u0026#34;) - ps: Invoke-Expression \u0026#34;git config --global user.name $env:git_name\u0026#34; 2\u0026gt;\u0026amp;1 - ps: Invoke-Expression \u0026#34;git config --global user.email $env:git_email\u0026#34; 2\u0026gt;\u0026amp;1 - ps: Invoke-Expression \u0026#34;git add --all\u0026#34; 2\u0026gt;\u0026amp;1 - ps: Invoke-Expression \u0026#34;git commit --allow-empty -m \u0026#39;Built from commit $revision\u0026#39;\u0026#34; 2\u0026gt;\u0026amp;1 - ps: Invoke-Expression \u0026#34;git push origin $env:target_branch\u0026#34; 2\u0026gt;\u0026amp;1 On the first two PowerShell lines, we set up our Git credentials to ensure we aren\u0026rsquo;t prompted for a password utilising our access_token variable we created earlier. We then grab the Git revision number and store it in a variable before creating a new directory and changing into it. Next up, we clone the master branch from our Github repo into the folder we\u0026rsquo;ve just changed into before recursively copying the \u0026lsquo;public\u0026rsquo; built site into the directory (excluding .git and appveyor.yml files).\nWe now have all of the latest built files copied into the freshly cloned master repo folder. The last thing for us to do is to add our username and email to our Git config, stage the files and commit with reference to the revision number and push it to our master branch.\nNote: You might be wondering why all of the Git commands are ran using the Invoke-Expression command and redirect standard error to standard output (2\u0026gt;\u0026amp;1). Without this, it seems that the Appveyor shell treats some of the output from Git as standard error which produces horrible red text and obviously stops the build. By utilising the redirect and Invoke-Expression, the build successfully runs without error.\nThat's all for now folks... I might come back and add a small section on setting up a custom domain at a later date. "
    }
]
